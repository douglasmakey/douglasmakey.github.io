[{"content":"As I experiment with Raspberry Pi and other devices in my network, I have created a small network application to aid in device discovery using multicast, data collection, and other functions.\nOne key feature of this application is the ability to download various data and metrics from some plugins weekly. With file sizes ranging from 200 MB to 250 MB after applying some compression, it\u0026rsquo;s essential to carefully consider some approaches for sending these files over TCP using Go.\nIn this article, we\u0026rsquo;ll explore some approaches and tips for sending large files over TCP in linux using Go, taking into account the constraints of small devices and the importance of efficient and reliable file transmission.\nNaive approach func sendFile(file *os.File, conn net.Conn) error { // Get file stat fileInfo, _ := file.Stat() // Send the file size sizeBuf := make([]byte, 8) binary.LittleEndian.PutUint64(sizeBuf, uint64(fileInfo.Size())) _, err := conn.Write(sizeBuf) if err != nil { return err } // Send the file contents by chunks buf := make([]byte, 1024) for { n, err := file.Read(buf) if err == io.EOF { break } _, err = conn.Write(buf[:n]) if err != nil { fmt.Println(\u0026#34;error writing to the conn:\u0026#34;, err) break } } return nil } Although this code appears straightforward, it has a significant drawback regarding efficiency. The code moves data in a loop from the kernel buffer for the source to a buffer in user space and then immediately copies it from that buffer to the kernel buffer for the destination. This double copying of data results in a loss of performance as the buffer serves only as a temporary holding place.\nWhile increasing the buf size to minimize the number of system calls might seem like a viable solution, it actually results in an increase in memory usage, making it an inefficient approach for tiny devices.\nMoreover, the double copying of data also increases memory usage, as both the source and destination buffers must be allocated and maintained in memory. This can strain the system\u0026rsquo;s resources, particularly when transferring large files and the devices are small.\nThe below diagram provides a simplified illustration of data flow when sending files over TCP. Using the previous approach, it\u0026rsquo;s important to note that the data is copied four times before the process is complete:\nFrom the disk to the read buffer in the kernel space. From the read buffer in the kernel space to the app buffer in the user space. From the app buffer in the user space to the socket buffer in the kernel space. Finally, from the socket buffer in the kernel space to the Network Interface Controller (NIC). This highlights the inefficiency of copying data multiple times, that\u0026rsquo;s without mention the multiple context switches between user mode and kernel mode.\nThe data is copied from the disk to the read buffer in kernel space when a read() call is issued, and the copy is performed by direct memory access (DMA). This results in a context switch from user mode to kernel mode. The data is then copied from the read buffer to the app buffer by the CPU, which requires another context switch from kernel to user mode.\nWhen a write/send() call is made, another context switch from user mode to kernel mode occurs, and the data is copied from the app buffer to a socket buffer in kernel space by the CPU. Then, a fourth context switch occurs as the write/send() call returns. The DMA engine then passes the data to the protocol engine asynchronously.\nWhat is DMA?\nDMA stands for Direct Memory Access. It\u0026rsquo;s a technology that allows peripheral devices to access computer memory directly, without needing the CPU, to speed up data transfer. In this way, the CPU is freed from performing the data transfer itself, allowing it to perform other tasks and making the system more efficient. https://en.wikipedia.org/wiki/Direct_memory_access\nTo optimize the file transfer process, we have to minimize the number of buffer copies and context switches and reduce the overhead of moving data from one place to another.\nUsing a specialized syscall \u0026lsquo;sendfile\u0026rsquo; Golang provides access to low-level operating system functionality through the syscall package, which contains an interface to various system primitives.\nfunc sendFile(file *os.File, conn net.Conn) error { // Get file stat fileInfo, _ := file.Stat() // Send the file size sizeBuf := make([]byte, 8) binary.LittleEndian.PutUint64(sizeBuf, uint64(fileInfo.Size())) if _, err := conn.Write(sizeBuf); err != nil { return err } tcpConn, ok := conn.(*net.TCPConn) if !ok { return errors.New(\u0026#34;TCPConn error\u0026#34;) } tcpF, err := tcpConn.File() if err != nil { return err } // Send the file contents _, err = syscall.Sendfile(int(tcpF.Fd()), int(file.Fd()), nil, int(fileInfo.Size())) return err } sendfile() copies data between one file descriptor and another. Because this copying is done within the kernel, sendfile() is more efficient than the combination of read(2) and write(2), which would require transferring data to and from user space. https://man7.org/linux/man-pages/man2/sendfile.2.html\nThe sendfile syscall is more efficient in transferring data than standard read and write methods. By bypassing the app buffer, the data moves directly from the read buffer to the socket buffer, reducing the number of data copies and context switches and improving performance. Furthermore, the process could requires less CPU intervention, allowing quicker data transfer and freeing up CPU for other tasks.\nThe sendfile syscall is known as a \u0026ldquo;zero-copy\u0026rdquo; method because it transfers data from one file descriptor to another without the need for an intermediate data copy in user-space memory.\nOf course this \u0026ldquo;zero-copy\u0026rdquo; is from a user-mode application point of view.\nThis scenario has two DMA copies + one CPU copy, and two context switches.\nThe sendfile syscall becomes even more efficient when the NIC supports Scatter/Gather. With SG, the syscall can directly transfer the data from the read buffer to the NIC, making the transfer a zero-copy operation that reduces the CPU load and enhances performance.\nGather refers to the ability of a Network Interface Card (NIC) to receive data from multiple memory locations and combine it into a single data buffer before transmitting it over the network. A NIC\u0026rsquo;s scatter/gather feature is used to increase the efficiency of data transfer by reducing the number of memory copies required to transmit the data. Instead of copying the data into a single buffer, the NIC can gather data from multiple buffers into a single buffer, reducing the CPU load and increasing the transfer\u0026rsquo;s performance. https://en.wikipedia.org/wiki/Gather/scatter_(vector_addressing)\nNic with gather supports\nThis scenario has just two DMA copies and two context switches.\nTherefore, reducing the number of buffer copies not only improves performance but also reduces memory usage, making the file transfer process more efficient and scalable.\nNote that the illustrations and scenarios provided are highly simplified and don\u0026rsquo;t fully represent the complexity of these processes. However, the aim was to present the information in a straightforward and easy-to-understand manner.\nWhy is \u0026ldquo;io.Copy\u0026rdquo; frequently recommended in Go? func sendFile(file *os.File, conn net.Conn) error { // Get file stat fileInfo, _ := file.Stat() // Send the file size sizeBuf := make([]byte, 8) binary.LittleEndian.PutUint64(sizeBuf, uint64(fileInfo.Size())) _, err := conn.Write(sizeBuf) if err != nil { return err } // Send the file contents _, err = io.Copy(conn, file) return err } The recommendation to use the io.Copy function in Go is due to its simplicity and efficiency. This function offers a streamlined way to copy data from an io.Reader to an io.Writer, managing buffering and chunking data to minimize memory usage and reduce syscalls. Additionally, io.Copy handles any potential errors during the copy process, making it a convenient and dependable option for data copying in Go.\nThe benefits of using io.Copy in Go go beyond its 32k buffer management and optimization src.\nfunc copyBuffer(dst Writer, src Reader, buf []byte) (written int64, err error) { ... if wt, ok := src.(WriterTo); ok { return wt.WriteTo(dst) } if rt, ok := dst.(ReaderFrom); ok { return rt.ReadFrom(src) } ... } When the destination satifies the ReadFrom interface, io.Copy utilizes this by calling ReadFrom to handle the copy process. For example, when dst is a TCPConn, io.Copy will call the underlying function to complete the copy src.\nfunc (c *TCPConn) readFrom(r io.Reader) (int64, error) { if n, err, handled := splice(c.fd, r); handled { return n, err } if n, err, handled := sendFile(c.fd, r); handled { return n, err } return genericReadFrom(c, r) } As you can see, when sending a file over a TCP connection, io.copy utilizes the sendfile syscall for efficient data transfer.\nBy running the program and using the strace tool to log all system calls, you can observe the use of the sendfile syscall in action:\n... [pid 67436] accept4(3, \u0026lt;unfinished ...\u0026gt; ... [pid 67440] epoll_pwait(5, \u0026lt;unfinished ...\u0026gt; [pid 67436] sendfile(4, 9, NULL, 4194304) = 143352 ... As observed in the implementation of ReadFrom, io.Copy not only attempts to use sendfile, but also the splice syscall, another useful system call for efficiently transferring data through pipes.\nIn addition, when the source satifies the WriteTo method, io.Copy will utilize it for the copy, avoiding any allocations and reducing the need for extra copying. This is why experts recommend using io.Copy whenever possible for copying or transferring data.\nPossible tips for Linux. I also try to improve performance on Linux systems for generic scenarios by increasing the MTU (Maximum Transmission Unit) size of the network interfaces and changing the TCP buffer size.\nThe Linux kernel parameters tcp_wmem and tcp_rmem control the transmit and receive buffer size for TCP connections, respectively. These parameters can be used to optimize the performance of TCP sockets.\ntcp_wmem determines the write buffer size for each socket, storing outgoing data before it is sent to the network. Larger buffers increase the amount of data sent at once, improving network efficiency.\ntcp_rmem sets the read buffer size for each socket, holding incoming data before the application processes it. This helps prevent network congestion and enhances efficiency.\nIncreasing both values will demand more memory usage.\nRead more.\n# See current tcp buffer values $ sysctl net.ipv4.tcp_wmem net.ipv4.tcp_wmem = 4096 16384 4194304 # Change the values $ sysctl -w net.ipv4.tcp_wmem=\u0026#34;X X X\u0026#34; # Change MTU $ ifconfig \u0026lt;Interface_name\u0026gt; mtu \u0026lt;mtu_size\u0026gt; up For me, these optimizations failed to deliver a substantial improvement due to certain constraints, such as the limitations of some devices, local network, etc.\nIn conclusion. The article discussed ways to send large files over TCP in Linux using Go, considering the constraints of small devices and the importance of efficient and reliable file transmission. The naive approach of copying data multiple times was deemed inefficient and increased memory usage, causing strain on the system\u0026rsquo;s resources. An alternative approach was presented, using the specialized syscall \u0026lsquo;sendfile\u0026rsquo; and, more importantly, io.Copy which use sendfile under the hood for this scenario to minimize the number of buffer copies and context switches and reduce overhead to achieve a more efficient file transfer.\nThank you for taking the time to read this article. I hope it provided some helpful information. I constantly work to improve my understanding and knowledge, so I appreciate your feedback or corrections. Thank you again for your time and consideration.\nRepo\n","permalink":"https://www.kungfudev.com/posts/optimizing-large-file-transfer-linux-go/","summary":"As I experiment with Raspberry Pi and other devices in my network, I have created a small network application to aid in device discovery using multicast, data collection, and other functions.\nOne key feature of this application is the ability to download various data and metrics from some plugins weekly. With file sizes ranging from 200 MB to 250 MB after applying some compression, it\u0026rsquo;s essential to carefully consider some approaches for sending these files over TCP using Go.","title":"Optimizing Large File Transfers in Linux with Go - An Exploration of TCP and Syscall"},{"content":"The Kubernetes crew just dropped the latest version, k8s 1.26 a few days ago, and it\u0026rsquo;s packed with some seriously cool new features. One that\u0026rsquo;s catching my eye is CEL for admission control - it allows us to create a ValidatingAdmissionPolicy, taking our cluster security to the next level.\nValidating admission policies offer a declarative, in-process alternative to validating admission webhooks.\nValidating admission policies use the Common Expression Language (CEL) to declare the validation rules of a policy.\nWhat is CEL? The Common Expression Language (CEL) implements common semantics for expression evaluation, enabling different applications to more easily interoperate. https://github.com/google/cel-spec\nIt is a programming language used in Kubernetes to create custom admission control policies. It allows for creating complex, fine-grained policies to enforce security and compliance in a cluster.\nIn a previous article, we explored how Kubernetes admission controllers can be used to enforce validations, such as preventing the use of the latest tag for container images. In this post, we\u0026rsquo;ll delve into the new ValidatingAdmissionPolicy feature, which allows us to take our validation capabilities even further in a much simple way without dealing with webhooks!\nTesting K8s 1.26 locally For local testing, you can use tools like kind or microk8s.\nBefore using either kind or microk8s, make sure to enable the ValidatingAdmissionPolicy feature gate and the admissionregistration.k8s.io/v1alpha1 API.\nmicrok8s - https://microk8s.io microk8s install --channel 1.26 You\u0026rsquo;ll need to edit the kube-apiserver file located at the address specified in the snippet below to activate the necessary features.\n$ vim /var/snap/microk8s/current/args/kube-apiserver ... --feature-gates=ValidatingAdmissionPolicy=true --runtime-config=admissionregistration.k8s.io/v1alpha1=true ... If you\u0026rsquo;re using a Mac and have microk8s installed, it uses multipass as VM. To access the VM, run the following command:\n$ multipass list Name State IPv4 Image microk8s-vm Running 192.168.64.5 Ubuntu 18.04 LTS $ multipass shell microk8s-vm kind - https://kind.sigs.k8s.io With kind, you can create a cluster using a configuration file written in YAML that contains all the necessary settings.\nkind: Cluster name: demo-cel apiVersion: kind.x-k8s.io/v1alpha4 featureGates: \u0026#34;ValidatingAdmissionPolicy\u0026#34;: true runtimeConfig: \u0026#34;admissionregistration.k8s.io/v1alpha1\u0026#34;: true nodes: - role: control-plane image: kindest/node:v1.26.0 Run\n$ kind create cluster --config=cluster.yaml For both kind and microk8s, you can use kubectl to check if all required features are enabled.\n$ kubectl api-versions | grep admissionregistration admissionregistration.k8s.io/v1alpha1 $ kubectl api-resources | grep ValidatingAdmissionPolicy validatingadmissionpolicies admissionregistration.k8s.io/v1alpha1 false ValidatingAdmissionPolicy validatingadmissionpolicybindings admissionregistration.k8s.io/v1alpha1 false ValidatingAdmissionPolicyBinding Now that you have your cluster set up, you are ready to start experimenting with ValidatingAdmissionPolicy.\nPlaying To create a policy, you need to create the following:\nA ValidatingAdmissionPolicy that outlines the abstract logic of the policy (e.g. \u0026ldquo;this policy ensures that a specific label is set to a specific value\u0026rdquo;). And a ValidatingAdmissionPolicyBinding that connects the ValidatingAdmissionPolicy to a specific scope or context.\u0026quot; Let\u0026rsquo;s say you want to create a policy that ensures high availability in production deployments by requiring a minimum of 3 replicas. This policy could be implemented using the following ValidatingAdmissionPolicy and ValidatingAdmissionPolicyBinding resources:\napiVersion: admissionregistration.k8s.io/v1alpha1 kind: ValidatingAdmissionPolicy metadata: name: \u0026#34;force-ha-in-prod\u0026#34; spec: failurePolicy: Fail matchConstraints: resourceRules: - apiGroups: [\u0026#34;apps\u0026#34;] apiVersions: [\u0026#34;v1\u0026#34;] operations: [\u0026#34;CREATE\u0026#34;, \u0026#34;UPDATE\u0026#34;] resources: [\u0026#34;deployments\u0026#34;] validations: - expression: \u0026#34;object.spec.replicas \u0026gt;= 3\u0026#34; message: \u0026#34;All production deployments should be HA with at least three replicas\u0026#34; The YAML above shows how you can specify which resources and operations will be affected by the policy. Within the spect.validations section, you can use CEL to define the expressions that the resource must satisfy to be accepted by the policy. If an expression evaluates to false, the validation check is enforced according to the spec.failurePolicy field.\nAccording to the official Kubernetes documentation, CEL expressions can access the contents of Admission requests and responses through a number of variables:\nobject: The object from the incoming request (null for DELETE requests). oldObject: The existing object (null for CREATE requests). request: Attributes of the admission request. params: The parameter resource referred to by the policy binding being evaluated (null if ParamKind is unset). Now that you have your first policy, you need to bind it using a ValidatingAdmissionPolicyBinding:\napiVersion: admissionregistration.k8s.io/v1alpha1 kind: ValidatingAdmissionPolicyBinding metadata: name: \u0026#34;force-ha-in-prod-binding\u0026#34; spec: policyName: \u0026#34;force-ha-in-prod\u0026#34; matchResources: namespaceSelector: matchLabels: env: production The MatchResources decides whether to run the admission control policy on an object based on whether it meets the match criteria. In this case, you\u0026rsquo;re using namespaceSelector to apply the policy to all namespaces labeled with env: production.\nMaybe you want to use objectSelector, which decides whether to run the validation based on if the object has matching labels. It is evaluated against both the old and new versions of an object, allowing you to set up validations that ensure that resources are not changed in certain ways.\napiVersion: admissionregistration.k8s.io/v1alpha1 kind: ValidatingAdmissionPolicyBinding metadata: name: \u0026#34;force-ha-in-prod-object-binding\u0026#34; spec: policyName: \u0026#34;force-ha-in-prod\u0026#34; matchResources: objectSelector: matchLabels: from: kungfudev Of course, you can also use an empty LabelSelector, which will match all objects. This can be useful if you want your policy to run on all resources, regardless of their labels.\napiVersion: admissionregistration.k8s.io/v1alpha1 kind: ValidatingAdmissionPolicyBinding metadata: name: \u0026#34;force-ha-in-prod-for-all-binding\u0026#34; spec: policyName: \u0026#34;force-ha-in-prod\u0026#34; matchResources: To test the policy you can try to apply the following YAML, which includes a namespace and a deployment with only two replicas.\napiVersion: v1 kind: Namespace metadata: name: payments-system labels: env: production --- apiVersion: apps/v1 kind: Deployment metadata: namespace: payments-system name: payment-gateway labels: app: nginx from: kungfudev spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.23.3 ports: - containerPort: 80 Testing it:\n$ kubectl apply -f force-ha-in-prod.yaml validatingadmissionpolicy.admissionregistration.k8s.io/force-ha-in-prod created $ kubectl apply -f force-ha-in-prod-binding.yaml validatingadmissionpolicybinding.admissionregistration.k8s.io/force-ha-in-prod-binding created $ kubectl apply -f payment-deployment.yaml The deployments \u0026#34;payment-gateway\u0026#34; is invalid: : ValidatingAdmissionPolicy \u0026#39;force-ha-in-prod\u0026#39; with binding \u0026#39;force-ha-in-prod-binding\u0026#39; denied request: All production deployments should be HA with at least three replicas The policy rejects the deployment because it does not meet the requirements defined in the policy\u0026rsquo;s validation. However, if you modify the deployment to use 4 replicas instead of 2, the policy should allow it.\n... spec: replicas: 4 selector: matchLabels: ... $ kubectl apply -f payment-deployment.yaml deployment.apps/payment-gateway created Wow, we just made sure all our production deployments are highly available with a few simple moves! Our policy is pretty basic for now, but there\u0026rsquo;s no stopping us from creating more complex validations. Imagine not allowing the \u0026ldquo;latest\u0026rdquo; tag, only allowing images from our own registry, or making sure all containers have CPU limits set. The possibilities are endless with the power of ValidatingAdmissionPolicy at our fingertips!\nBy harnessing the full capabilities of CEL, we can chain multiple expressions to create more intricate validations.\nTo learn about additional features and capabilities offered by CEL, please visit this link.\napiVersion: admissionregistration.k8s.io/v1alpha1 kind: ValidatingAdmissionPolicy metadata: name: \u0026#34;prod-ready-policy\u0026#34; spec: failurePolicy: Fail matchConstraints: resourceRules: - apiGroups: [\u0026#34;apps\u0026#34;] apiVersions: [\u0026#34;v1\u0026#34;] operations: [\u0026#34;CREATE\u0026#34;, \u0026#34;UPDATE\u0026#34;] resources: [\u0026#34;deployments\u0026#34;] validations: - expression: \u0026#34;object.spec.template.spec.containers.all(c, !c.image.endsWith(\u0026#39;:latest\u0026#39;))\u0026#34; message: \u0026#34;cannot use the latest tag\u0026#34; - expression: \u0026#34;object.spec.template.spec.containers.all(c, c.image.startsWith(\u0026#39;myregistry.com/\u0026#39;))\u0026#34; message: \u0026#34;image from an untrusted registry\u0026#34; - expression: \u0026#34;has(object.metadata.labels.env) \u0026amp;\u0026amp; object.metadata.labels.env in [\u0026#39;prod\u0026#39;, \u0026#39;production\u0026#39;]\u0026#34; message: \u0026#34;deployment should have an env and have to be prod or production\u0026#34; - expression: | object.spec.template.spec.containers.all( c, has(c.resources) \u0026amp;\u0026amp; has(c.resources.limits) \u0026amp;\u0026amp; has(c.resources.limits.cpu) ) message: \u0026#34;container does not have a cpu limit set\u0026#34; --- apiVersion: admissionregistration.k8s.io/v1alpha1 kind: ValidatingAdmissionPolicyBinding metadata: name: \u0026#34;prod-reay-policy-binding\u0026#34; spec: policyName: \u0026#34;prod-reay-policy\u0026#34; matchResources: namespaceSelector: matchExpressions: - key: \u0026#34;env\u0026#34; operator: \u0026#34;In\u0026#34; values: [prod, production,] In the YAML above, you\u0026rsquo;ll notice that we can use matchExpressions during binding to give our policy a more dynamic scope.\nThe ValidatingAdmissionPolicy feature also includes Parameter resources, which allow you to separate the policy configuration from its definition. This means that, for example, instead of hardcoding the minimum number of replicas for all namespaces in the HA Policy, you can use a parameter and configure different values for different environments. Check out the official documentation for more information on how to use this feature.\nIn Conclusion With CEL and the new ValidatingAdmissionPolicy feature in k8s 1.26, we now have the power to create custom, complex policies to enforce security and compliance in our clusters.\nWe know that Open Policy Agent (OPA) is widely used, but ValidatingAdmissionPolicy + CEL is designed to be more lightweight and easier to use than OPA. ValidatingAdmissionPolicy is natively integrated with Kubernetes, which means that it can be used directly within the Kubernetes API server without requiring a separate policy engine.\nThere are pros and cons to using either OPA or ValidatingAdmissionPolicy + CEL for policy enforcement in Kubernetes. OPA could be a more feature-rich and powerful policy engine, but it may require more setup and maintenance. ValidatingAdmissionPolicy + CEL is easier to use and integrates more seamlessly with Kubernetes, but it may not have all the advanced features of OPA. Ultimately, the choice of which policy engine to use will depend on the specific requirements and preferences of the user.\nFind all the YAML files and even more examples in this repository.\nThanks for following along, and I hope you found this article helpful and easy to understand. Happy cluster building!\n","permalink":"https://www.kungfudev.com/posts/cel-for-admission-controller/","summary":"The Kubernetes crew just dropped the latest version, k8s 1.26 a few days ago, and it\u0026rsquo;s packed with some seriously cool new features. One that\u0026rsquo;s catching my eye is CEL for admission control - it allows us to create a ValidatingAdmissionPolicy, taking our cluster security to the next level.\nValidating admission policies offer a declarative, in-process alternative to validating admission webhooks.\nValidating admission policies use the Common Expression Language (CEL) to declare the validation rules of a policy.","title":"CEL for admission controller with ValidatingAdmissionPolicy in K8s 1.26"},{"content":"In my previous post, Understanding Unix Domain Sockets in Golang, I mentioned that one potential use case for Unix domain sockets is to communicate between containers in Kubernetes. I received requests for an example of how to do this, so in this post, I\u0026rsquo;ll provide a simple example using two Go applications that you can find in this repository.\nUsing Unix domain sockets in Kubernetes can be an effective way to communicate containers within the same pod.\nSome advantages of using Unix domain sockets for communication between containers within a pod in K8s are:\nFaster communication than using network sockets. This can be useful when containers need to communicate frequently or transfer large amounts of data. No need for a network interface Secure transmission, the communication is restricted to the local host. Simplicity, no need for IP addresses or port numbers. Ok, this maybe isn\u0026rsquo;t a great advantage but YOLO! In K8s, you can achieve this by sharing a volume between the containers and using the socket file within the volume as the communication channel.\nSharing a Volume in Kubernetes: To share the Unix domain socket file between two containers in the same pod, you must create an emptyDir volume and mount it in both containers. In Kubernetes, you can do this using a volumeMount in the container specification in the yaml.\nAn emptyDir volume is a temporary volume created when a Pod is assigned to a node and exists as long as that Pod runs on that node. An emptyDir volume is initialized with an empty directory and can be used to store data shared between the containers in the Pod.\nHere is an example of how you might create a volume and mount it in two containers in the same pod:\napiVersion: apps/v1 kind: Deployment metadata: name: kungfudev-deploy spec: replicas: 1 selector: matchLabels: app: my-app template: metadata: labels: app: my-app spec: containers: - name: nethttp image: douglasmakey/simple-http:latest volumeMounts: - mountPath: /tmp/ name: socket-volume - name: unixhttp image: douglasmakey/simple-http-uds:latest volumeMounts: - mountPath: /tmp/ name: socket-volume volumes: - name: socket-volume emptyDir: {} In this example, both nethttp and unixhttp have a volume mounted in the container’s filesystem in /tmp, which allows them to access the same files within the volume.\nRunning Ready to try this in your own Kubernetes cluster? Check out this repository, which has everything you need to get started! The included Earthfile in the repo will help you build and create container images for use in your local K8s cluster, or you can simply use the images published on Docker Hub. Follow the example YAML above to set up your own Unix domain socket communication between containers.\nEarthly is a tool for building, testing, and deploying applications using containers. It provides a simple and easy-to-use command-line interface for defining and automating the build, test, and deployment steps of a project using a script called an Earthfile.\nOnce the deployment is up and running on your K8s cluster, it\u0026rsquo;s time to put it to the test! Head over to the nethttp container and make a request to /test using curl to see how it performs.\n$ k exec -it kungfudev-deploy -c nethttp bash --- $ curl localhost:8000/test Hello kung fu developer from a server running on UDS! Show me the code! The unixhttp app code:\npackage main import (...) const socketPath = \u0026#34;/tmp/httpecho.sock\u0026#34; func main() { // Create a Unix domain socket and listen for incoming connections. socket, err := net.Listen(\u0026#34;unix\u0026#34;, socketPath) if err != nil { log.Fatal(err) } // Cleanup the sockfile. c := make(chan os.Signal, 1) signal.Notify(c, os.Interrupt, syscall.SIGTERM) go func() { \u0026lt;-c os.Remove(socketPath) os.Exit(1) }() m := http.NewServeMux() m.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\u0026#34;Hi kung fu developer from a server running on UDS! \\n\u0026#34;)) }) server := http.Server{Handler: m} if err := server.Serve(socket); err != nil { log.Fatal(err) } } The nethttp app code:\npackage main import (...) var ( socketPath = \u0026#34;/tmp/httpecho.sock\u0026#34; // Creating a new HTTP client that is configured to make HTTP requests over a Unix domain socket. httpClient = http.Client{ Transport: \u0026amp;http.Transport{ // Set the DialContext field to a function that creates // a new network connection to a Unix domain socket DialContext: func(_ context.Context, _, _ string) (net.Conn, error) { return net.Dial(\u0026#34;unix\u0026#34;, socketPath) }, }, } ) func test(w http.ResponseWriter, req *http.Request) { resp, err := httpClient.Get(\u0026#34;http://unix/\u0026#34;) if err != nil { http.Error(w, err.Error(), 500) } defer resp.Body.Close() b, err := io.ReadAll(resp.Body) if err != nil { http.Error(w, err.Error(), 500) } w.Write(b) } func main() { http.HandleFunc(\u0026#34;/test\u0026#34;, test) if err := http.ListenAndServe(\u0026#34;:8000\u0026#34;, nil); err != nil { log.Fatal(err) } } That\u0026rsquo;s it! With just a few simple steps, you can leverage the power of Unix domain sockets to communicate between containers in a Kubernetes pod. This may be a simple example, but the technique can be applied to all sorts of real-world scenarios. Thanks for following along, and I hope you found this example helpful and easy to understand. Happy coding!\n","permalink":"https://www.kungfudev.com/posts/simple-example-of-using-unix-domain-socket-in-kubernetes/","summary":"In my previous post, Understanding Unix Domain Sockets in Golang, I mentioned that one potential use case for Unix domain sockets is to communicate between containers in Kubernetes. I received requests for an example of how to do this, so in this post, I\u0026rsquo;ll provide a simple example using two Go applications that you can find in this repository.\nUsing Unix domain sockets in Kubernetes can be an effective way to communicate containers within the same pod.","title":"A simple example of using unix domain socket in Kubernetes"},{"content":"In Golang, a socket is a communication endpoint that allows a program to send and receive data over a network. There are two main types of sockets in Golang: Unix domain sockets (AF_UNIX) and network sockets (AF_INET|AF_INET6). This blog post will explore some differences between these two types of sockets.\nUnix domain sockets, a.k.a., local sockets, are used for communication between processes on the same machine. They use a file-based interface and can be accessed using the file system path, just like regular files. In some cases, Unix domain sockets are faster and more efficient than network sockets as they do not require the overhead of network protocols and communication. They are commonly used for interprocess communication (IPC) and communication between services on the same machine. The data is transmitted between processes using the file system as the communication channel.\nThe file system provides a reliable and efficient mechanism for transmitting data between processes. Only the kernel is involved in the communication between processes. The processes communicate by reading and writing to the same socket file, which is managed by the kernel. The kernel is responsible for handling the communication details, such as synchronization, buffering and error handling, and ensures that the data is delivered reliably and in the correct order.\nThis is different from network sockets, where the communication involves the kernel, the network stack, and the network hardware. The processes use network protocols, such as TCP/UDP, in network sockets to establish connections and transfer data over the network. The kernel and the network stack handle the communication details, such as routing, addressing, and error correction. The network hardware handles the physical transmission of the data over the network.\nIn Golang, Unix domain sockets are created using the net.Dial \u0026ldquo;client\u0026rdquo; or net.Listen \u0026ldquo;server\u0026rdquo; functions, with the unix network type. For example, the following code creates a Unix domain socket and listens for incoming connections:\n// Create a Unix domain socket and listen for incoming connections. socket, err := net.Listen(\u0026#34;unix\u0026#34;, \u0026#34;/tmp/mysocket.sock\u0026#34;) if err != nil { panic(err) } Let\u0026rsquo;s look at an example of how to use Unix domain sockets in Golang. The following code creates a simple echo server using a Unix domain socket:\npackage main import (...) func main() { // Create a Unix domain socket and listen for incoming connections. socket, err := net.Listen(\u0026#34;unix\u0026#34;, socketPath) if err != nil { log.Fatal(err) } // Cleanup the sockfile. c := make(chan os.Signal, 1) signal.Notify(c, os.Interrupt, syscall.SIGTERM) go func() { \u0026lt;-c os.Remove(\u0026#34;/tmp/echo.sock\u0026#34;) os.Exit(1) }() for { // Accept an incoming connection. conn, err := socket.Accept() if err != nil { log.Fatal(err) } // Handle the connection in a separate goroutine. go func(conn net.Conn) { defer conn.Close() // Create a buffer for incoming data. buf := make([]byte, 4096) // Read data from the connection. n, err := conn.Read(buf) if err != nil { log.Fatal(err) } // Echo the data back to the connection. _, err = conn.Write(buf[:n]) if err != nil { log.Fatal(err) } }(conn) } } Let\u0026rsquo;s test the above echo server using netcat, you can use the -U option to specify the socket file. This allows you to connect to the socket and send and receive data through it.\n$ echo \u0026#34;I\u0026#39;m a Kungfu Dev\u0026#34; | nc -U /tmp/echo.sock I\u0026#39;m a Kungfu Dev Network sockets, on the other hand, are used for communication between processes on different machines. They use network protocols, such as TCP and UDP. Network sockets are more versatile than Unix domain sockets, as they can be used to communicate with processes on any machine that is connected to the network. They are commonly used for client-server communication, such as web servers and client applications.\nIn Golang, network sockets are created using the net.Dial or net.Listen functions, with a network type such as TCP or UDP. For example, the following code creates a TCP socket and listens for incoming connections:\n// Create a TCP socket and listen for incoming connections. socket, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;:8000\u0026#34;) if err != nil { panic(err) } Basic Profiling from a client perspective Client pprof for network socket\nType: cpu ... (pprof) list main.main ... ROUTINE ======================== main.main in /Users/douglasmakey/go/src/github.com/douglasmakey/go-sockets-uds-network-pprof/server_echo_network_socket/client/main.go 0 530ms (flat, cum) 70.67% of Total . . 16: . . 17: pprof.StartCPUProfile(f) . . 18: defer pprof.StopCPUProfile() . . 19: . . 20: for i := 0; i \u0026lt; 10000; i++ { . 390ms 21: conn, err := net.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;localhost:3000\u0026#34;) . . 22: if err != nil { . . 23: log.Fatal(err) . . 24: } . . 25: . . 26: msg := \u0026#34;Hello\u0026#34; . 40ms 27: if _, err := conn.Write([]byte(msg)); err != nil { . . 28: log.Fatal(err) . . 29: } . . 30: . . 31: b := make([]byte, len(msg)) . 100ms 32: if _, err := conn.Read(b); err != nil { . . 33: log.Fatal(err) . . 34: } . . 35: } . . 36:} Client pprof for unix socket\nType: cpu ... (pprof) list main.main ... ROUTINE ======================== main.main in /Users/douglasmakey/go/src/github.com/douglasmakey/go-sockets-uds-network-pprof/server_echo_unix_domain_socket/client/main.go 0 210ms (flat, cum) 80.77% of Total . . 16: . . 17: pprof.StartCPUProfile(f) . . 18: defer pprof.StopCPUProfile() . . 19: . . 20: for i := 0; i \u0026lt; 10000; i++ { . 130ms 21: conn, err := net.Dial(\u0026#34;unix\u0026#34;, \u0026#34;/tmp/echo.sock\u0026#34;) . . 22: if err != nil { . . 23: log.Fatal(err) . . 24: } . . 25: . . 26: msg := \u0026#34;Hello\u0026#34; . 40ms 27: if _, err := conn.Write([]byte(msg)); err != nil { . . 28: log.Fatal(err) . . 29: } . . 30: . . 31: b := make([]byte, len(msg)) . 40ms 32: if _, err := conn.Read(b); err != nil { . . 33: log.Fatal(err) . . 34: } . . 35: } . . 36:} Things to notice in these basic profiles are:\nOpen an unix socket is significantly faster than a network socket. Reading from unix socket is significantly faster than reading from a network socket. Github Repo\nUse Unix domain socket with HTTP Server To use a Unix domain socket with an HTTP server in Go, you can use the server.Serve function and specify the net.Listener to listen on.\npackage main import (...) const socketPath = \u0026#34;/tmp/httpecho.sock\u0026#34; func main() { // Create a Unix domain socket and listen for incoming connections. socket, err := net.Listen(\u0026#34;unix\u0026#34;, socketPath) if err != nil { panic(err) } // Cleanup the sockfile. c := make(chan os.Signal, 1) signal.Notify(c, os.Interrupt, syscall.SIGTERM) go func() { \u0026lt;-c os.Remove(socketPath) os.Exit(1) }() m := http.NewServeMux() m.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\u0026#34;Hello kung fu developer! \u0026#34;)) }) server := http.Server{ Handler: m, } if err := server.Serve(socket); err != nil { log.Fatal(err) } } Test it\n$ curl -s -N --unix-socket /tmp/httpecho.sock http://localhost/ Hello kung fu developer! Using an HTTP server with a Unix domain socket in Go can provide several advantages, which were named in this article, such as improved security, performance, ease of use, and interoperability. These advantages can make the HTTP server implementation more efficient, reliable, and scalable for processes that have to communicate between the same machine.\nWhat about security? Unix domain sockets and network sockets have different security characteristics. In general, Unix domain sockets are considered to be more secure than network sockets, as they are not exposed to the network and are only accessible to processes on the same machine.\nOne of the main security features of Unix domain sockets is that they use file system permissions to control access. The socket file is created with a specific user and group and can only be accessed by processes that have the correct permissions for that user and group. This means that only authorized processes can connect to the socket and exchange data.\nIn contrast, network sockets are exposed to the network and are accessible to any machine that is connected to the network. This makes them vulnerable to attacks from malicious actors, such as hackers and malware. Network sockets use network protocols, such as TCP/UDP. These protocols have their own security mechanisms, such as encryption and authentication. However, these mechanisms are not always sufficient to protect against all types of attacks, and network sockets can still be compromised.\nWhich to choose? When deciding between Unix domain sockets and network sockets in Golang, it is important to consider the requirements and constraints of the application. Unix domain sockets are faster and more efficient but are limited to communication between processes on the same machine. Network sockets are more versatile but require more overhead and are subject to network latency and reliability issues. In general, Unix domain sockets are suitable for IPC and communication between services on the same machine, while network sockets are suitable for client-server communication over the network.\nYou should choose a Unix domain socket over a network socket when you need to communicate between processes on the same host. Unix domain sockets provide a secure and efficient communication channel between processes on the same host and are suitable for scenarios where the processes need to exchange data frequently or in real time.\nFor example, imagine you have two containers in a K8s pod, which must communicate with each other as quickly as possible!\nIn conclusion, Unix domain sockets and network sockets are two important types of sockets in Golang and are used for different purposes and scenarios. Understanding the differences and trade-offs between these types of sockets is essential for designing and implementing efficient and reliable networked applications in Golang.\nRemember, the communication between processes with a unix socket is handled solely by the kernel, while in a network socket, the communication involves the kernel, the network stack, and the network hardware.\n","permalink":"https://www.kungfudev.com/posts/understanding-unix-domain-sockets-in-golang/","summary":"In Golang, a socket is a communication endpoint that allows a program to send and receive data over a network. There are two main types of sockets in Golang: Unix domain sockets (AF_UNIX) and network sockets (AF_INET|AF_INET6). This blog post will explore some differences between these two types of sockets.\nUnix domain sockets, a.k.a., local sockets, are used for communication between processes on the same machine. They use a file-based interface and can be accessed using the file system path, just like regular files.","title":"Understanding Unix Domain Sockets in Golang"},{"content":"I bet there have been many times that you were working on the terminal with multiple tabs and you launched an HTTP server, and then you forgot that the server was already being executed, and then you tried to relaunch it from another tab getting the known error:\ngo run main.go listen tcp :8080: bind: address already in use This is because we cannot open a socket with the same source address and port by default in Linux and the vast majority of operating systems.\nSocket options When we create a new TCP socket on Linux, we can set options that affect the behaviour of the socket. For example, one of these options is SO_REUSEPORT, which allows multiple sockets to bind to the same IP address and port. With this feature, the Linux kernel distributes incoming requests across all the sockets that share the same address and port combination, getting a load balancing inside the Kernel.\nSO_REUSEPORT\nFor TCP sockets, this option allows accept(2) load distribution in a multi-threaded server to be improved by using a distinct listener socket for each thread. This provides improved load distribution as compared to traditional techniques such using a single accept(2)ing thread that distributes connections, or having multiple threads that compete to accept(2) from the same socket.\nFor UDP sockets, the use of this option can provide better distribution of incoming datagrams to multiple processes (or threads) as compared to the traditional technique of having multiple processes compete to receive datagrams on the same socket.\nhttps://man7.org/linux/man-pages/man7/socket.7.html\nAs we can notice, we not only get the super power to create more than one socket with the same IP: Port combination, but we also obtain a kind of load balancer in the kernel mode.\nGo sockets When we invoke the net.Listen() function in Go, this function use the ListenConfig struct to create the Listener.\nfunc Listen(network, address string) (Listener, error) { var lc ListenConfig return lc.Listen(context.Background(), network, address) } If we inspect ListenConfig we can find the method Control and the documentation says: If Control is not nil, it is called after creating the network connection but before binding it to the operating system.\n// ListenConfig contains options for listening to an address. type ListenConfig struct { // If Control is not nil, it is called after creating the network // connection but before binding it to the operating system. // ... Control func(network, address string, c syscall.RawConn) error ... } The function Control receives the syscall.RawConn which is a raw network connection that has a method also called control (Control(f func(fd uintptr))) where it will invoke the function f on the underlying connection\u0026rsquo;s file descriptor.\nHaving the file descriptor, now we can use the golang.org/x/sys/unix package to set the socket options.\n// fd -\u0026gt; the underlying connection\u0026#39;s file descriptor. // unix.SOL_SOCKET -\u0026gt; to set options at the socket level, we have to specify the level argument as SOL_SOCKET. unix.SetsockoptInt(int(fd), unix.SOL_SOCKET, unix.SO_REUSEPORT, 1) So, we could create a instance of ListenConfig with a control function to set the SO_REUSEPORT socket options to our sockets.\nvar lc = net.ListenConfig{ Control: func(network, address string, c syscall.RawConn) error { var opErr error if err := c.Control(func(fd uintptr) { opErr = unix.SetsockoptInt(int(fd), unix.SOL_SOCKET, unix.SO_REUSEPORT, 1) }); err != nil { return err } return opErr }, } Security One question we might have at this point is, what about security? I mean, if we can open a socket with the same IP: Port of a specific app, for example, Nginx, we could hijack part of the requests that the kernel will send to us through the socket. Right?\nWell, to prevent this \u0026ldquo;port hijacking,\u0026rdquo; Linux has special protections or mechanisms to prevent these problems, such as:\nBoth sockets must have been created with the SO_REUSEPORT socket option. If there is a socket running without SO_REUSEPORT and we try to create another socket even with the SO_REUSEPORT socket option, it will fail with the error already in use. All sockets that want to listen to the same IP and port combination must have the same effective userID. For example, if you want to hijack the Nginx port and it is running under the ownership of the user Pepito, a new process can listen to the same port only if it is also owned by the user Pepito. So one user cannot \u0026ldquo;steal\u0026rdquo; ports of other users. The following are super simple use cases for SO_REUSEPORT, of course omitting all the complexity required to achieve them:\nWe could run multiple instances of our app to take advantage of our resources without the necessity of running a proxy in front of them (to have an ultra simple LB). Having multiple threads/processes/instances will have better performance than having a single one. Can give us the possibility of zero downtime updates. Since we can launch a new instance to receive requests and, after that, kill the old one with a graceful shutdown. Simple demo In this repository, you will find the complete code example in Go to test this, but as it is a fairly simple code and something short, you will also have it below:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;syscall\u0026#34; \u0026#34;golang.org/x/sys/unix\u0026#34; ) var lc = net.ListenConfig{ Control: func(network, address string, c syscall.RawConn) error { var opErr error if err := c.Control(func(fd uintptr) { opErr = unix.SetsockoptInt(int(fd), unix.SOL_SOCKET, unix.SO_REUSEPORT, 1) }); err != nil { return err } return opErr }, } func main() { pid := os.Getpid() l, err := lc.Listen(context.Background(), \u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:8080\u0026#34;) if err != nil { panic(err) } server := \u0026amp;http.Server{} http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { w.WriteHeader(http.StatusOK) fmt.Fprintf(w, \u0026#34;Hello from PID %d \\n\u0026#34;, pid) }) fmt.Printf(\u0026#34;HTTP Server with PID: %d is running \\n\u0026#34;, pid) panic(server.Serve(l)) } Using the above code, we can open a terminal with 3 tabs. In the first one, we will run the program:\n$ go run main.go HTTP Server with PID: 8183 is running In the second one, we will have another instance of our program.\n$ go run main.go HTTP Server with PID: 8298 is running Then, in the last one, we will run a simple loop to hit our servers, and you should have a similar result:\n$ for i in {1..20}; do curl localhost:8080; done Hello from PID 8183 Hello from PID 8183 Hello from PID 8183 Hello from PID 8183 Hello from PID 8183 Hello from PID 8183 Hello from PID 8183 Hello from PID 8183 Hello from PID 8183 Hello from PID 8298 Hello from PID 8298 Hello from PID 8183 Hello from PID 8183 Hello from PID 8183 Hello from PID 8298 Hello from PID 8298 Hello from PID 8298 Hello from PID 8183 Hello from PID 8183 Hello from PID 8298 I hope you enjoyed this little article, for me this was very interesting and that is why I decided to share it with you \u0026lt;3\nRelated link:\nhttps://lwn.net/Articles/542629/ https://man7.org/linux/man-pages/man7/socket.7.html https://www.nginx.com/blog/socket-sharding-nginx-release-1-9-1/ ","permalink":"https://www.kungfudev.com/posts/socket-sharding-go/","summary":"I bet there have been many times that you were working on the terminal with multiple tabs and you launched an HTTP server, and then you forgot that the server was already being executed, and then you tried to relaunch it from another tab getting the known error:\ngo run main.go listen tcp :8080: bind: address already in use This is because we cannot open a socket with the same source address and port by default in Linux and the vast majority of operating systems.","title":"Socket sharding in Linux example with Go"},{"content":"What is an admission controller? In a nutshell, Kubernetes admission controllers are plugins that govern and enforce how the cluster is used. They can be thought of as a gatekeeper that intercept (authenticated) API requests and may change the request object or deny the request altogether. The admission control process has two phases: the mutating phase is executed first, followed by the validating phase.\nKubernetes admission Controller Phases:\nAn admission controller is a piece of software that intercepts requests to the Kubernetes API server before the persistence of the object (the k8s resource such as Pod, Deployment, Service, etc\u0026hellip;) in the etcd database, but after the request is authenticated and authorized.\nWith the admission controller, we can validate or \u0026ldquo;mutate\u0026rdquo; the resource on the incoming requests. For example, imagine the following use cases:\nYou want to apply a simple security validation that none of the containers in your cluster could use the latest tag. You may want to set some default values such as annotations or labels in every resource that you deploy. There are two types of admission controllers in Kubernetes. They are validating admission controller and mutating admission controller. Mutating admission controllers are invoked first and can \u0026ldquo;modify\u0026rdquo; objects. After all object modifications are complete, and after the incoming object is validated by the API server, validating admission controllers are invoked. They can reject requests to enforce custom policies.\nNote: I put the words \u0026ldquo;modify\u0026rdquo; and \u0026ldquo;mutate\u0026rdquo; in quotes because we don\u0026rsquo;t really modify the resource itself. We tell Kubernetes what it should modify to the object \u0026ldquo;the K8s resource\u0026rdquo; using JSON Patch format.\nA mutating admission controller can act as a mutating or validating controller. It can perform both actions to the request simultaneously. However, remember that the mutating admission controllers are executed first. To ensure that you will validate an object\u0026rsquo;s last state, you should use the validating admission controller.\nRegister our admission controller webhooks I created this repository that has all the code for this example and a simple boilerplate for an admission controller in Go GitHub.\nA cluster on which this example can be tested must be running Kubernetes 1.9.0 or above. Also it should have the admissionregistration.k8s.io/v1beta1 API enabled. You can verify that using the following command:\nkubectl api-versions ... admissionregistration.k8s.io/v1beta1 ... You should check that MutatingAdmissionWebhook and ValidatingAdmissionWebhook are activated in your cluster inspecting the kube-apiserver.\n--enable-admission-plugins=..,MutatingAdmissionWebhook,ValidatingAdmissionWebhook..\u0026#34; To implement our admission controller, the Kubernetes API server needs to know when and where to send the incoming requests to our admission controller. We have to create aValidatingWebhookConfiguration or MutatingWebhookConfiguration object in Kubernetes depends on what we want.\nFor example, the following configuration is to register a ValidatingWebhookConfiguration to apply some validations to creating a Pod.\napiVersion: admissionregistration.k8s.io/v1beta1 kind: ValidatingWebhookConfiguration metadata: name: pod-validation webhooks: - name: pod-validation.default.svc clientConfig: service: name: admission-server namespace: default path: \u0026#34;/validate/pods\u0026#34; caBundle: \u0026#34;${CA_BUNDLE}\u0026#34; rules: - operations: [\u0026#34;CREATE\u0026#34;] apiGroups: [\u0026#34;\u0026#34;] apiVersions: [\u0026#34;v1\u0026#34;] resources: [\u0026#34;pods\u0026#34;] The main parts here are:\nclientConfig: The config for our admission controller server. service.name: The name of the service for our admission controller server. service.namespace: In what namespace our admission controller server is. service.path: The path where our admission controller will receive this webhook request. rules: Contains the operations, resources, resources\u0026rsquo; api versions and groups that you want to intercept operations: The operations you wish to intercept for these resources. For example, CREATE, DELETE, UPDATED resources: The resources you want to intercept on this webhook. For example, Pods, Deployment, Service, etc. apiGroups and apiVersions of these resources. The Kubernetes API server makes an HTTPS POST request to the given service and URL path. Since a webhook must be served via HTTPS, we need proper certificates for the server. These certificates can be self-signed, but we need Kubernetes to instruct the respective CA certificate when talking to the webhook server. For that, you see the caBundle in the configuration.\nIn the GitHub repository, you will find the demo/deploy.sh script that will create a self-signed certificates and create the webhooks in demo/webhooks.yaml for this example.\nTo correctly manage your certificates in production, you could use something like a https://cert-manager.io.\nCreate our admission controller server Note: All the code examples here have been simplified to make them easier to read. For the full implementation, please visit the repository.\nLet’s start creating a simple HTTPS. It should have an endpoint for the path that we defined for our admission webhook.\n// http/server.go func NewServer(port string) *http.Server { // Instances hooks podsValidation := pods.NewValidationHook() // Routers ah := newAdmissionHandler() mux := http.NewServeMux() mux.Handle(\u0026#34;/validate/pods\u0026#34;, ah.Serve(podsValidation)) // The path of the webhook for Pod validation. return \u0026amp;http.Server{ Addr: fmt.Sprintf(\u0026#34;:%s\u0026#34;, port), Handler: mux, } } // cmd/main.go func main() { // flags // ... server := http.NewServer(port) if err := server.ListenAndServeTLS(tlscert, tlskey); err != nil { log.Errorf(\u0026#34;Failed to listen and serve: %v\u0026#34;, err) } } Then we have to create the admissionHandler to receive all the requests from our webhooks. These requests are coming with a JSON-encoded AdmissionReview (with the Request field filled) in the request body. The response should be a JSON AdmissionReview with the Response field filled.\n// http/handlers.go type admissionHandler struct {...} // Serve returns a http.HandlerFunc for an admission webhook func (h *admissionHandler) Serve(hook admissioncontroller.Hook) http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { // HTTP validations // ... body, err := io.ReadAll(r.Body) if err != nil {...} var review admission.AdmissionReview if _, _, err := h.decoder.Decode(body, nil, \u0026amp;review); err != nil {...} result, err := hook.Execute(review.Request) if err != nil {...} admissionResponse := v1beta1.AdmissionReview{ Response: \u0026amp;v1beta1.AdmissionResponse{ UID: review.Request.UID, Allowed: result.Allowed, Result: \u0026amp;meta.Status{Message: result.Msg}, }, } //... res, err := json.Marshal(admissionResponse) if err != nil {...} w.WriteHeader(http.StatusOK) w.Write(res) } } As you noticed in the code above, our handler receives a Hook struct and invokes its Execute method to process the request. In the Hook struct, we can register an AdmitFunc for each operation allowed.\n// AdmitFunc defines how to process an admission request type AdmitFunc func(request *admission.AdmissionRequest) (*Result, error) // Hook represents the set of functions for each operation in an admission webhook. type Hook struct { Create AdmitFunc Delete AdmitFunc Update AdmitFunc Connect AdmitFunc } // Execute evaluates the request and try to execute the function for operation specified in the request. func (h *Hook) Execute(r *admission.AdmissionRequest) (*Result, error) { switch r.Operation { case admission.Create: return wrapperExecution(h.Create, r) ..... } return \u0026amp;Result{Msg: fmt.Sprintf(\u0026#34;Invalid operation: %s\u0026#34;, r.Operation)}, nil } Now, we are ready to just focus on our business logic. We need to write the code for the webhook that we registered.\nWe will implement the logic for our ValidatingWebhook that we registered for the CREATE operation in the Pod resource. We want to validate that none of the pod\u0026rsquo;s containers could use the latest tag in its image.\n// pods/pods.go // NewValidationHook creates a new instance of pods validation hook func NewValidationHook() admissioncontroller.Hook { return admissioncontroller.Hook{ Create: validateCreate(), } } // validateImages validates that none of the containers use the `latest` tag. func validateImages() admissioncontroller.AdmitFunc { return func(r *v1beta1.AdmissionRequest) (*admissioncontroller.Result, error) { pod, err := parsePod(r.Object.Raw) if err != nil { return \u0026amp;admissioncontroller.Result{Msg: err.Error()}, nil } for _, c := range pod.Spec.Containers { if strings.HasSuffix(c.Image, \u0026#34;:latest\u0026#34;) { return \u0026amp;admissioncontroller.Result{Msg: \u0026#34;You cannot use the tag \u0026#39;latest\u0026#39; in a container.\u0026#34;}, nil } } return \u0026amp;admissioncontroller.Result{Allowed: true}, nil } } Of course, this is a very basic example, and you could implement more complex validations depends on your use cases. For instance, on the repository, we have another interesting example. Using a MutattingWebhook and JSON Patch, we inject a container to our pod as a sidecar.\nNote: Istio uses a similar approach to inject its sidecar containers.\nfunc mutateCreate() admissioncontroller.AdmitFunc { return func(r *v1beta1.AdmissionRequest) (*admissioncontroller.Result, error) { var operations []admissioncontroller.PatchOperation pod, err := parsePod(r.Object.Raw) if err != nil { return \u0026amp;admissioncontroller.Result{Msg: err.Error()}, nil } // Very simple logic to inject a new \u0026#34;sidecar\u0026#34; container. if pod.Namespace == \u0026#34;special\u0026#34; { var containers []v1.Container containers = append(containers, pod.Spec.Containers...) sideC := v1.Container{ Name: \u0026#34;test-sidecar\u0026#34;, Image: \u0026#34;busybox:stable\u0026#34;, Command: []string{\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;while true; do echo \u0026#39;I am a container injected by mutating webhook\u0026#39;; sleep 2; done\u0026#34;}, } containers = append(containers, sideC) operations = append(operations, admissioncontroller.ReplacePatchOperation(\u0026#34;/spec/containers\u0026#34;, containers)) } return \u0026amp;admissioncontroller.Result{ Allowed: true, PatchOps: operations, }, nil } } Deploy and test Run demo/deploy.sh will create a self-signed CA, a certificate, and private for the server and the webhooks. It also will create the following resources:\nSecret TLS The Deployment of our admission server using the demo/deployment.yaml A Service for our admission server All the Admission webhooks Note: demo/deploy.sh is just for develop/test environment. It was not intended for production.\nYou can see all the created resources:\nkubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE admission-server ClusterIP 10.43.120.27 \u0026lt;none\u0026gt; 443/TCP 1h kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE admission-server 1/1 0 1 1h kubectl get secret NAME TYPE DATA AGE admission-tls kubernetes.io/tls 2 1h kubectl get mutatingwebhookconfigurations NAME WEBHOOKS AGE pod-mutation 1 1h kubectl get validatingwebhookconfigurations NAME WEBHOOKS AGE deployment-validation 1 1h pod-validation 1 1h Now, we can test our webhook. If we try to create a Pod using the following manifest will fail.\n# demo/pods/01_fail_pod_creation_test.yaml apiVersion: v1 kind: Pod metadata: name: webserver spec: containers: - name: webserver image: nginx:latest ports: - containerPort: 80 Note: You can use the different manifests inside demo/pods and demo/deployments to test the validations and mutations.\nkubectl create -f pods/01_fail_pod_creation_test.yaml Error from server: error when creating \u0026#34;pods/01_fail_pod_creation_test.yaml\u0026#34;: admission webhook \u0026#34;pod-validation.default.svc\u0026#34; denied the request: You cannot use the tag \u0026#39;latest\u0026#39; in a container. Conclusion As you can see, the admission controller is a powerful feature that allows us to implement custom rules and default values to the k8s resources. It is one way you can extend the k8s behavior.\nI hope you have enjoyed this article, and any feedback will be very well received.\nIf you are a Go developer or a Kubernetes ecosystem fan and want to work on many exciting stuff. In that case, we are hiring in my team at Ubisoft.\nMany thanks.\nSome interesting links https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/#why-do-i-need-admission-controllers https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/ https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ http://jsonpatch.com ","permalink":"https://www.kungfudev.com/posts/implementing-k8s-admission-controller/","summary":"What is an admission controller? In a nutshell, Kubernetes admission controllers are plugins that govern and enforce how the cluster is used. They can be thought of as a gatekeeper that intercept (authenticated) API requests and may change the request object or deny the request altogether. The admission control process has two phases: the mutating phase is executed first, followed by the validating phase.\nKubernetes admission Controller Phases:\nAn admission controller is a piece of software that intercepts requests to the Kubernetes API server before the persistence of the object (the k8s resource such as Pod, Deployment, Service, etc\u0026hellip;) in the etcd database, but after the request is authenticated and authorized.","title":"Implementing a simple K8s admission controller in Go"},{"content":"A few days ago, I was reading about the Kubernetes network model, especially about services and the kube-proxy component, and I discovered that kube-proxy has three modes, which are userspace, iptables and ipvs.\nThe userspace mode is too old and slow, nowaday nobody recommends to use it, the iptables mode is the default mode for kube-proxy with this mode kube-proxy use iptables rules to forward packets that are destined for services to a backend for that services, and the last one is ipvs I did not know what it was so I read about it.\nWhat is IPVS? IPVS (IP Virtual Server) is built on top of the Netfilter and implements transport-layer load balancing as part of the Linux kernel.\nIPVS is incorporated into the LVS (Linux Virtual Server), where it runs on a host and acts as a load balancer in front of a cluster of real servers. IPVS can direct requests for TCP- and UDP-based services to the real servers, and make services of the real servers appear as virtual services on a single IP address.\nThat means IPVS is a Linux kernel load balancer over layer 4, if you don\u0026rsquo;t know what is the difference between LB L4 and LB L7 there is a good explanation Here.\nLB L4\nAt Layer 4, a load balancer has visibility on network information such as application ports and protocol (TCP/UDP). The load balancer delivers traffic by combining this limited network information with a load balancing algorithm such as round-robin and by calculating the best destination server based on least connections or server response times.\nSo in this layer, you are not parsing the data in the packages, so you don\u0026rsquo;t know what\u0026rsquo;s inside, for instance, if you are using LB 4 and receive an HTTP request at this layer you can\u0026rsquo;t see the path or the body o headers of this request so you cant take a smart decision based on this.\nLB L7\nAt Layer 7, a load balancer has application awareness and can use this additional application information to make more complex and informed load balancing decisions. With a protocol such as HTTP, a load balancer can uniquely identify client sessions based on cookies and use this information to deliver all a clients requests to the same server. This server persistence using cookies can be based on the server’s cookie or by active cookie injection where a load balancer cookie is inserted into the connection. Free LoadMaster includes cookie injection as one of many methods of ensuring session persistence.\nSimple Demo Now you know what is IPVS, we are going to make an ultra-simple demo using docker to have an LB with IPVS between two containers.\nThe first thing that we need is the CLI tool for interacting with the IP virtual server table in the kernel.\nipvsadm - Linux Virtual Server administration ipvsadm.\nsudo apt-get install -y ipvsadm Create the virtual service. Now, we can use the CLI to create a new virtual service:\nipvsadm COMMAND [protocol] service-address [scheduling-method] [persistence options] If we see in the documentation of ipvsadm, we will see that using the flag -A we indicated \u0026ldquo;Add a virtual service\u0026rdquo;, and the flag -s is for the scheduling-method, first we will try with rr that means Round Robin, we have different options such as: wrr - Weighted Round Robin, lc - Least-Connection, lblc - Locality-Based Least-Connection and more.\nSo we are creating the virtual service for the address 100.100.100.100:80 using Round Robin as scheduling-method.\nsudo ipvsadm -A -t 100.100.100.100:80 -s rr Create two docker container We are going to use the image jwilder/whoami for our containers, this image just returns the container\u0026rsquo;s id.\n$ docker run -d -p 8000:8000 --name first -t jwilder/whoami cd977829ae0c76236a1506c497d5ce1628f1f701f8ed074916b21fc286f3d0d1 $ docker run -d -p 8001:8000 --name second -t jwilder/whoami 5886b1ed7bd4095cb02b32d1642866095e6f4ce1750276bd9fc07e91e2fbc668 Then, we are going to get IP of these containers, using docker inspect\n$ docker inspect -f \u0026#39;{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}\u0026#39; first 172.17.0.2 $ docker inspect -f \u0026#39;{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}\u0026#39; second 172.17.0.3 Using curl to one of these containers, we will see the container_id.\n$ curl 172.17.0.2:8000 I\u0026#39;m cd977829ae0c Add the IPs to the virtual service. We have the containers\u0026rsquo; IP, so we are going to add these IP to the virtual service using ipvsadm with the flags -a to add a server to the virtual service that we specified using -t and -m to use masquerading (network access translation, or NAT).\n$ sudo ipvsadm -a -t 100.100.100.100:80 -r 172.17.0.2:8000 -m $ sudo ipvsadm -a -t 100.100.100.100:80 -r 172.17.0.3:8000 -m We can use the ipvsadm to list the virtual services with its servers.\n$ ipvsadm -l IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u0026gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 100.100.100.100:http rr -\u0026gt; 172.17.0.2:8000 Masq 1 0 0 -\u0026gt; 172.17.0.3:8000 Masq 1 0 0 If you added a wrong server, you can remove that server with -d flag.\n$ ipvsadm -d -t 100.100.100.100:http -r 172.17.0.3:8000 Now our service can make loadbalacing over L4 using Round Robin as algorithm to balance.\n$ curl 100.100.100.100 I\u0026#39;m 5886b1ed7bd4 $ curl 100.100.100.100 I\u0026#39;m cd977829ae0c $ curl 100.100.100.100 I\u0026#39;m 5886b1ed7bd4 $ curl 100.100.100.100 I\u0026#39;m cd977829ae0c As you can see, doing load balancing with IPVS is pretty straightforward.\nIn K8S one of the advantages of choosing IPVS mode for kube-proxy instead of iptables is that IPVS is a Linux kernel feature that is designed for load balancing, it has multiple different scheduling algorithms such as round-robin, shortest-expected-delay, least connections and more, also it has an optimized look-up routine O(1) based on a hash table data structure rather than a list of sequential rules O(n) \u0026ldquo;iptables adds the rules in a sequential chain that grows roughly in proportion to the number of services and number of backend pods behind each service).\u0026rdquo;\n","permalink":"https://www.kungfudev.com/posts/how-to-setup-simple-load-balancing-with-ipvs-demo-with-docker/","summary":"A few days ago, I was reading about the Kubernetes network model, especially about services and the kube-proxy component, and I discovered that kube-proxy has three modes, which are userspace, iptables and ipvs.\nThe userspace mode is too old and slow, nowaday nobody recommends to use it, the iptables mode is the default mode for kube-proxy with this mode kube-proxy use iptables rules to forward packets that are destined for services to a backend for that services, and the last one is ipvs I did not know what it was so I read about it.","title":"How to setup simple load balancing with IPVS, demo with docker"},{"content":"A few days ago, I read an article about BigCache and I was interested to know how they avoided these 2 problems:\nconcurrent access expensive GC cycles I went to their repository and read the code to understand how they achieved it. I think it\u0026rsquo;s amazing so I would like to share it with you.\n\u0026lsquo;Fast, concurrent, evicting in-memory cache written to keep big number of entries without impact on performance. BigCache keeps entries on heap but omits GC for them. To achieve that operations on bytes arrays take place, therefore entries (de)serialization in front of the cache will be needed in most use cases.\u0026rsquo;\nBigCache\nConcurrent access Surely you will need concurrent access, either your program uses goroutines, or you have an HTTP server that allocates goroutines for each request. The most common approach to achieve it would be to use sync.RWMutex in front of the cache access function to ensure that only one goroutine could modify it at a time, but if you use this approach and other goroutine try to make modifications in the cache, the second goroutine would be blocked until the first goroutine unlock the lock, causing undesirable contention periods.\nTo solve this problem, they used shards, but what is a shard? A shard is a struct that contains its instance of the cache with a lock.\nThen they use an array of N shards to distribute the data into them, so when you are going to put or get data from the cache, a shard for that data is chosen by a function that we will talk later, in this way the locks contention can be minimized, because each shard has its lock.\ntype cacheShard struct { items map[uint64]uint32 lock sync.RWMutex array []byte tail int } Expensive GC cycles var map[string]Item The most common pattern in a simple implementation of cache in Go is using a map to save the items, but if you are using a map the garbage collector (GC) will touch every single item of that map during the mark phase, this can be very expensive on the application performance when the map is very large.\nAfter go version 1.5, if you use a map without pointers in keys and values, the GC will omit its content.\nvar map[int]int To avoid this, they used a map without pointers in keys and values, with this the GC will omit the entries in the map and use an array of bytes, where they can put the entry serialized in bytes, then they can store in the map the hashedkey like key and the index of the entry into the array like the value.\nUsing an array of bytes is a smart solution because it only adds one additional object to the mark phase. Since a byte array doesn’t have any pointers (other than the object itself), the GC can mark the entire object in O(1) time.\nLet\u0026rsquo;s start coding It will be a fairly simple implementation of cache, I avoided eviction, capacity and other things, the code will be simple just to demonstrate how they solved the problems I talked above.\nFirst, the hasher this is a copy \u0026amp; paste from their repository, you can find the code Here, it is a Hasher which makes no memory allocations.\nhasher.go\npackage main // newDefaultHasher returns a new 64-bit FNV-1a Hasher which makes no memory allocations. // Its Sum64 method will lay the value out in big-endian byte order. // See https://en.wikipedia.org/wiki/Fowler–Noll–Vo_hash_function func newDefaultHasher() fnv64a { return fnv64a{} } type fnv64a struct{} const ( // offset64 FNVa offset basis. See https://en.wikipedia.org/wiki/Fowler–Noll–Vo_hash_function#FNV-1a_hash offset64 = 14695981039346656037 // prime64 FNVa prime value. See https://en.wikipedia.org/wiki/Fowler–Noll–Vo_hash_function#FNV-1a_hash prime64 = 1099511628211 ) // Sum64 gets the string and returns its uint64 hash value. func (f fnv64a) Sum64(key string) uint64 { var hash uint64 = offset64 for i := 0; i \u0026lt; len(key); i++ { hash ^= uint64(key[i]) hash *= prime64 } return hash } Second, the cache struct contains the logic to get the shards and functions get\u0026amp;set.\nI talked above in the Concurrent access section about a function to choose a shard for the data, to achived this they use the hasher above to hash the key and with the hashedkey get a shard for the the key, to achived that they do a bitwise operation with AND operator, using a mask based on the size of shards to turn off certain bits to get a value into the range of shards.\nhashedkey\u0026amp;mask 0111 AND 1101 (mask) = 0101 cache.go\npackage main var minShards = 1024 type cache struct { shards []*cacheShard hash fnv64a } func newCache() *cache { cache := \u0026amp;cache{ hash: newDefaultHasher(), shards: make([]*cacheShard, minShards), } for i := 0; i \u0026lt; minShards; i++ { cache.shards[i] = initNewShard() } return cache } func (c *cache) getShard(hashedKey uint64) (shard *cacheShard) { return c.shards[hashedKey\u0026amp;uint64(minShards-1)] } func (c *cache) set(key string, value []byte) { hashedKey := c.hash.Sum64(key) shard := c.getShard(hashedKey) shard.set(hashedKey, value) } func (c *cache) get(key string) ([]byte, error) { hashedKey := c.hash.Sum64(key) shard := c.getShard(hashedKey) return shard.get(key, hashedKey) } Finally, where the magic occurs, in each shard have an array of bytes []byte and a map map[uint64]uint32. In the map, they put the index for each entry like value and in the array save the entry in bytes.\nThey use the tail to keep the index in the array of bytes.\nshard.go\npackage main import ( \u0026#34;encoding/binary\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;sync\u0026#34; ) const ( headerEntrySize = 4 defaultValue = 1024 // For this example we use 1024 like default value. ) type cacheShard struct { items map[uint64]uint32 lock sync.RWMutex array []byte tail int headerBuffer []byte } func initNewShard() *cacheShard { return \u0026amp;cacheShard{ items: make(map[uint64]uint32, defaultValue), array: make([]byte, defaultValue), tail: 1, headerBuffer: make([]byte, headerEntrySize), } } func (s *cacheShard) set(hashedKey uint64, entry []byte) { w := wrapEntry(entry) s.lock.Lock() index := s.push(w) s.items[hashedKey] = uint32(index) s.lock.Unlock() } func (s *cacheShard) push(data []byte) int { dataLen := len(data) index := s.tail s.save(data, dataLen) return index } func (s *cacheShard) save(data []byte, len int) { // Put in the first 4 bytes the size of the value binary.LittleEndian.PutUint32(s.headerBuffer, uint32(len)) s.copy(s.headerBuffer, headerEntrySize) s.copy(data, len) } func (s *cacheShard) copy(data []byte, len int) { // Using the tail to keep the order to write in the array s.tail += copy(s.array[s.tail:], data[:len]) } func (s *cacheShard) get(key string, hashedKey uint64) ([]byte, error) { s.lock.RLock() itemIndex := int(s.items[hashedKey]) if itemIndex == 0 { s.lock.RUnlock() return nil, errors.New(\u0026#34;key not found\u0026#34;) } // Read the first 4 bytes after the index, remember these 4 bytes have the size of the value, so // you can use this to get the size and get the value in the array using index+blockSize to know until what point // you need to read blockSize := int(binary.LittleEndian.Uint32(s.array[itemIndex : itemIndex+headerEntrySize])) entry := s.array[itemIndex+headerEntrySize : itemIndex+headerEntrySize+blockSize] s.lock.RUnlock() return readEntry(entry), nil } func readEntry(data []byte) []byte { dst := make([]byte, len(data)) copy(dst, data) return dst } func wrapEntry(entry []byte) []byte { // You can put more information like a timestamp if you want. blobLength := len(entry) blob := make([]byte, blobLength) copy(blob, entry) return blob } main.go\npackage main import \u0026#34;fmt\u0026#34; func main() { cache := newCache() cache.set(\u0026#34;key\u0026#34;, []byte(\u0026#34;the value\u0026#34;)) value, err := cache.get(\u0026#34;key\u0026#34;) if err != nil { fmt.Println(err) } fmt.Println(string(value)) // OUTPUT: // the value } Github Repo\n","permalink":"https://www.kungfudev.com/posts/how-bigcache-avoids-expensive-gc-cycles-and-speeds-up-concurrent-access-in-go/","summary":"A few days ago, I read an article about BigCache and I was interested to know how they avoided these 2 problems:\nconcurrent access expensive GC cycles I went to their repository and read the code to understand how they achieved it. I think it\u0026rsquo;s amazing so I would like to share it with you.\n\u0026lsquo;Fast, concurrent, evicting in-memory cache written to keep big number of entries without impact on performance.","title":"How BigCache avoids expensive GC cycles and speeds up concurrent access in Go"},{"content":"Simple implementation of Dijkstra using heap in Go. What is Dijkstra? MEGA SHORT DESCRIPTION: Dijkstra\u0026rsquo;s algorithm to find the shortest path between a and b. It picks the unvisited node with the lowest distance, calculates the distance through it to each unvisited neighbor, and updates the neighbor\u0026rsquo;s distance if smaller.\nMark all nodes unvisited. Create a set of all the unvisited nodes called the unvisited set, in our case we are going to use a set for visited nodes, not for unvisited nodes.\nAssign to every node a tentative distance value: set it to zero for our initial node. Set the initial node as current.\nFor the current node, consider all of its unvisited neighbors and calculate their tentative distances through the current node. Compare the newly calculated tentative distance to the current assigned value and assign the smaller one. For example, if the current node A is marked with a distance of 6, and the edge connecting it with a neighbor B has length 2, then the distance to B through A will be 6 + 2 = 8. If B was previously marked with a distance greater than 8 then change it to 8. Otherwise, keep the current value.\nWhen we are done considering all of the unvisited neighbors of the current node, mark the current node as visited. A visited node will never be checked again.\nSelect next unvisited node that is marked with the smallest tentative distance, set it as the new \u0026ldquo;current node\u0026rdquo;, and go back to step 3.\nWikipedia\nWhat is Heap? In computer science, a heap is a specialized tree-based data structure which is essentially an almost complete tree that satisfies the heap property: in a max heap, for any given node C, if P is a parent node of C, then the key (the value) of P is greater than or equal to the key of C. In a min heap, the key of P is less than or equal to the key of C The node at the \u0026ldquo;top\u0026rdquo; of the heap (with no parents) is called the root node.\nWikipedia\nA heap can be thought of as a priority queue; the most important node will always be at the top, and when removed, its replacement will be the most important. This can be useful when coding algorithms that require certain things to processed in a complete order, but when you don\u0026rsquo;t want to perform a full sort or need to know anything about the rest of the nodes. For instance, a well-known algorithm for finding the shortest distance between nodes in a graph, Dijkstra\u0026rsquo;s Algorithm, can be optimized by using a priority queue.\nCprogramming\nWhy? I am trying to learn about graphs and its algorithms because I never went to the university so I don\u0026rsquo;t know much about graph, because of that I try to read and learn about this in my free time, I recently watched a video about one implementation of Dijkstra in python using a heap was interesting, so I decided to do the same but with go.\nI know that there are many articles about the same topic and these articles explain very well what is Dijkstra or what is a heap, this article will be a short article just focus on the implementation, I want to show you a very simple implementation of Dijkstra using heap in Golang.\nIf you want to read more about Dijkstra you should read this article that I found is amazing.\nAn excelenct article\nImplementation Dijkstra is an algorithm for searching the short path between two nodes, visiting the neighbors of each node and calculating the cost and the path from origin node keeping always the smallest value, for that we can use a min-heap to keep the min value in each iteration, using push and pop operation, both operations are O(log n).\nFirst, we need to implement for min-heap, golang has a package in its standard library for that.\nthe Package heap provides heap operations for any type that implements heap.Interface. A heap is a tree with the property that each node is the minimum-valued node in its subtree.\nGodoc - heap\nheap.go\npackage main import hp \u0026#34;container/heap\u0026#34; type path struct { value int nodes []string } type minPath []path func (h minPath) Len() int { return len(h) } func (h minPath) Less(i, j int) bool { return h[i].value \u0026lt; h[j].value } func (h minPath) Swap(i, j int) { h[i], h[j] = h[j], h[i] } func (h *minPath) Push(x interface{}) { *h = append(*h, x.(path)) } func (h *minPath) Pop() interface{} { old := *h n := len(old) x := old[n-1] *h = old[0 : n-1] return x } type heap struct { values *minPath } func newHeap() *heap { return \u0026amp;heap{values: \u0026amp;minPath{}} } func (h *heap) push(p path) { hp.Push(h.values, p) } func (h *heap) pop() path { i := hp.Pop(h.values) return i.(path) } Secondly, we need to implement the logic for the graph, for that, we use a struct that contains a map to keep the edges among the nodes, with functions to add the edges and get all edges from one node.\nThe function getPath implement the Dijkstra algorithm to get the shortest path between origin and destiny.\ngraph.go\npackage main type edge struct { node string weight int } type graph struct { nodes map[string][]edge } func newGraph() *graph { return \u0026amp;graph{nodes: make(map[string][]edge)} } func (g *graph) addEdge(origin, destiny string, weight int) { g.nodes[origin] = append(g.nodes[origin], edge{node: destiny, weight: weight}) g.nodes[destiny] = append(g.nodes[destiny], edge{node: origin, weight: weight}) } func (g *graph) getEdges(node string) []edge { return g.nodes[node] } func (g *graph) getPath(origin, destiny string) (int, []string) { h := newHeap() h.push(path{value: 0, nodes: []string{origin}}) visited := make(map[string]bool) for len(*h.values) \u0026gt; 0 { // Find the nearest yet to visit node p := h.pop() node := p.nodes[len(p.nodes)-1] if visited[node] { continue } if node == destiny { return p.value, p.nodes } for _, e := range g.getEdges(node) { if !visited[e.node] { // We calculate the total spent so far plus the cost and the path of getting here h.push(path{value: p.value + e.weight, nodes: append([]string{}, append(p.nodes, e.node)...)}) } } visited[node] = true } return 0, nil } main.go\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { fmt.Println(\u0026#34;Dijkstra\u0026#34;) // Example graph := newGraph() graph.addEdge(\u0026#34;S\u0026#34;, \u0026#34;B\u0026#34;, 4) graph.addEdge(\u0026#34;S\u0026#34;, \u0026#34;C\u0026#34;, 2) graph.addEdge(\u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, 1) graph.addEdge(\u0026#34;B\u0026#34;, \u0026#34;D\u0026#34;, 5) graph.addEdge(\u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;, 8) graph.addEdge(\u0026#34;C\u0026#34;, \u0026#34;E\u0026#34;, 10) graph.addEdge(\u0026#34;D\u0026#34;, \u0026#34;E\u0026#34;, 2) graph.addEdge(\u0026#34;D\u0026#34;, \u0026#34;T\u0026#34;, 6) graph.addEdge(\u0026#34;E\u0026#34;, \u0026#34;T\u0026#34;, 2) fmt.Println(graph.getPath(\u0026#34;S\u0026#34;, \u0026#34;T\u0026#34;)) } $ go run . Dijkstra 12 [S C B D E T] Github\n","permalink":"https://www.kungfudev.com/posts/implementation-of-dijkstra-using-heap-in-go/","summary":"Simple implementation of Dijkstra using heap in Go. What is Dijkstra? MEGA SHORT DESCRIPTION: Dijkstra\u0026rsquo;s algorithm to find the shortest path between a and b. It picks the unvisited node with the lowest distance, calculates the distance through it to each unvisited neighbor, and updates the neighbor\u0026rsquo;s distance if smaller.\nMark all nodes unvisited. Create a set of all the unvisited nodes called the unvisited set, in our case we are going to use a set for visited nodes, not for unvisited nodes.","title":"Implementation of Dijkstra using heap in Go"},{"content":"In my company we have an ETL wrote in Golang to process the integrations with our partners, each integration is executed in an unique and isolate POD using cronjob k8s, each one print a bunch of data and metrics for each step executed using log the package in the standard library, all these logs are useful to monitor the integrations with different tools.\nIn my team now we want to receive an email when some integration is failed with the logs of the process, so for that, we use a feature of log to change the output destination for the standard logger called SetOutput.\nUsing io.MultiWriter we can create a writer combine multiple writers, in this case, we will combine a buffer with the standard os.Stderr to save the logs into the buffer and keep logs in stderr for monitorization.\nThe package log use os.Stderr for default.\nbuf := new(bytes.Buffer) w := io.MultiWriter(buf, os.Stderr) log.SetOutput(w) Then, all our logs are into the buffer, so we can create a file to save it in a temporal file and attach it to an email.\nf, err := os.Create(\u0026#34;/path/log.txt\u0026#34;) if err != nil { // Handler .... } defer f.Close() // We copy the buffer into the file. if _, err := io.Copy(f, buf); err != nil { // Handler .... } Now we can send an email with the logs file, I found some implementations to send an email with attachment file using only the standard libraries but they didn\u0026rsquo;t work, so I combine different approach and got this implementation.\npackage main import ( \u0026#34;bytes\u0026#34; \u0026#34;encoding/base64\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;mime/multipart\u0026#34; \u0026#34;net/smtp\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; ) var ( host = os.Getenv(\u0026#34;EMAIL_HOST\u0026#34;) username = os.Getenv(\u0026#34;EMAiL_USERNAME\u0026#34;) password = os.Getenv(\u0026#34;EMAIL_PASSWORD\u0026#34;) portNumber = os.Getenv(\u0026#34;EMAIL_PORT\u0026#34;) ) type Sender struct { auth smtp.Auth } type Message struct { To []string Subject string Body string Attachments map[string][]byte } func New() *Sender { auth := smtp.PlainAuth(\u0026#34;\u0026#34;, username, password, host) return \u0026amp;Sender{auth}, nil } func (s *Sender) Send(m *Message) error { return smtp.SendMail(fmt.Sprintf(\u0026#34;%s:%s\u0026#34;, host, portNumber), s.auth, username, m.To, m.ToBytes()) } func NewMessage(s, b string) *Message { return \u0026amp;Message{Subject: s, Body: b, Attachments: make(map[string][]byte)} } func (m *Message) AttachFile(src string) error { b, err := ioutil.ReadFile(src) if err != nil { return err } _, fileName := filepath.Split(src) m.Attachments[fileName] = b return nil } func (m *Message) ToBytes() []byte { buf := bytes.NewBuffer(nil) withAttachments := len(m.Attachments) \u0026gt; 0 buf.WriteString(fmt.Sprintf(\u0026#34;Subject: %s\\n\u0026#34;, m.Subject)) buf.WriteString(\u0026#34;MIME-Version: 1.0\\n\u0026#34;) writer := multipart.NewWriter(buf) boundary := writer.Boundary() if withAttachments { buf.WriteString(fmt.Sprintf(\u0026#34;Content-Type: multipart/mixed; boundary=%s\\n\u0026#34;, boundary)) buf.WriteString(fmt.Sprintf(\u0026#34;--%s\\n\u0026#34;, boundary)) } buf.WriteString(\u0026#34;Content-Type: text/plain; charset=utf-8\\n\u0026#34;) buf.WriteString(m.Body) if withAttachments { for k, v := range m.Attachments { buf.WriteString(fmt.Sprintf(\u0026#34;\\n\\n--%s\\n\u0026#34;, boundary)) buf.WriteString(\u0026#34;Content-Type: application/octet-stream\\n\u0026#34;) buf.WriteString(\u0026#34;Content-Transfer-Encoding: base64\\n\u0026#34;) buf.WriteString(fmt.Sprintf(\u0026#34;Content-Disposition: attachment; filename=%s\\n\u0026#34;, k)) b := make([]byte, base64.StdEncoding.EncodedLen(len(v))) base64.StdEncoding.Encode(b, v) buf.Write(b) buf.WriteString(fmt.Sprintf(\u0026#34;\\n--%s\u0026#34;, boundary)) } buf.WriteString(\u0026#34;--\u0026#34;) } return buf.Bytes() } func main() { sender := New() m := NewMessage(\u0026#34;Test\u0026#34;, \u0026#34;Body message.\u0026#34;) m.To = []string{\u0026#34;to@gmail.com\u0026#34;} m.AttachFile(\u0026#34;/path/to/file\u0026#34;) fmt.Println(s.Send(m)) } If you have some tips to improve this implementation or one way to do better that would be amazing \u0026hellip;\nGithub.\n","permalink":"https://www.kungfudev.com/posts/golang-capturing-logs-and-send-an-email/","summary":"In my company we have an ETL wrote in Golang to process the integrations with our partners, each integration is executed in an unique and isolate POD using cronjob k8s, each one print a bunch of data and metrics for each step executed using log the package in the standard library, all these logs are useful to monitor the integrations with different tools.\nIn my team now we want to receive an email when some integration is failed with the logs of the process, so for that, we use a feature of log to change the output destination for the standard logger called SetOutput.","title":"Golang: capturing logs and send an email"},{"content":"According to the azure documentation in this excellent article, they state that.\n\u0026ldquo;It\u0026rsquo;s a good practice, and often a business requirement, to monitor web applications and back-end services, to ensure they\u0026rsquo;re available and performing correctly. However, it\u0026rsquo;s more difficult to monitor services running in the cloud than it is to monitor on-premises services.\u0026rdquo;\n\u0026ldquo;There are many factors that affect cloud-hosted applications such as network latency, the performance, and availability of the underlying compute and storage systems and the network bandwidth between them. The service can fail entirely or partially due to any of these factors. Therefore, you must verify at regular intervals that the service is performing correctly to ensure the required level of availability.\u0026rdquo;\nWhen we work with multiple microservices deployed in a container orchestrator, we have a problem which is \u0026ldquo;How to detect that a running microservice instance is unable to handle requests?\u0026rdquo;.\nSolution Implement health monitoring by sending requests to an endpoint on the application. The application should perform the necessary checks, and return an indication of its status \u0026ldquo;health checks\u0026rdquo; normally returns 200 if all ok and 503 if the service is failing.\nThis will be a basic introduction to health checks.\nWhat are health checks? Health checks are basically endpoints provided by a microservice (e.g. HTTP /health) to check whether the service is running properly.\nWhy should we use health checks? All microservices should implement health checks. These checks can be used by orchestration tools \u0026ldquo;as a K8s\u0026rdquo; to kill an instance or raise an alert to monitoring tool in case of a failing health check.\nWhat can we check in health checks? Everything will depend on what do out service or what is our requirements.\nFor example, if our service using PostgreSQL to persist data or use Redis to cache, we need to ensure that our service can communicate with our storage services \u0026ldquo;as a PostgreSQL or Redis\u0026rdquo; because our logic depends on these storage services. if we can\u0026rsquo;t communicate with the database our service cannot work.\nAnother example is if our microservice receives files, such as images, stores them on disk, we need to check that we have available space on disk. Otherwise, our microservice we will not work.\nThe most important cases to check are:\nthe status of the connections to the infrastructure services used by the service instance the status of the others microservices, if it is required. the status of the host, e.g. disk space application specific logic I have seen a lot of projects that just implementing health check to return a response with status 200, without doing any checks. For example:\nfunc HealthCheck(w http.ResponseWriter, r *http.Request) { w.WriteHeader(http.StatusOK) return } Please, don\u0026rsquo;t do this!\nThe health check is a powerful ally that allows us to do a lot of things to check the behavior of our microservice and it can avoid us headaches if is combined with some monitoring tool.\nI will use my last articles where I wrote a service to look for a driver like uber to implement an example of the health check.\nTracking Service\nTracking Service V2\nWe need to create a handler for healthcheck, in this handler we will implement all the checks that we want to do.\nIf you already read my past articles you have seen that my microservice use redis for register location of drivers and also it uses redis to search drivers with the command \u0026lsquo;georadius\u0026rsquo; function of redis, so my microservice depends 100% on redis, means that I need to check my microservice can communicate with redis.\nFor this task I use the command Ping of redis, this command is used to test if a connection is still alive, or to measure latency.\nFile handler/healthcheck.go\nfunc health(w http.ResponseWriter, r *http.Request) { if r.Method != \u0026#34;GET\u0026#34; { w.WriteHeader(http.StatusMethodNotAllowed) return } // Get instance redis client redis := storages.GetRedisClient() // Checks that the communication with redis is alive. if err := redis.Ping().Err(); err != nil { // Put yours logs HERE log.Printf(\u0026#34;redis unaccessible error: %v \u0026#34;, err) w.WriteHeader(http.StatusServiceUnavailable) } else { w.WriteHeader(http.StatusOK) } return } So next we need to add our healtcheck to router.\nfunc NewHandler() *http.ServeMux { mux := http.NewServeMux() // Add healthcheck mux.HandleFunc(\u0026#34;/health\u0026#34;, health) // .... mux.HandleFunc(\u0026#34;/tracking\u0026#34;, tracking) mux.HandleFunc(\u0026#34;/search\u0026#34;, search) // V2 mux.HandleFunc(\u0026#34;/v2/search\u0026#34;, v2.SearchV2) mux.HandleFunc(\u0026#34;/v2/cancel\u0026#34;, v2.CancelRequest) return mux } Now we run service and run a container on docker with redis.\ndocker run -p 6379:6379 -d redis go run main.go Use curl to consume our new endpoint health to know our service is OK.\ncurl -X GET -I localhost:8000/health 1212:50:15 HTTP/1.1 200 OK Date: Sat, 12 Jan 2019 15:50:23 GMT Content-Length: 0 But if we stop the container with redis and try to hit to \u0026lsquo;/health\u0026rsquo; the healthcheck response should be 503 Service Unavailable because our service cant communicates with redis \u0026ldquo;for we stopped the container with Redis.\u0026rdquo;\ncurl -X GET -I localhost:8000/health 1313:38:24 HTTP/1.1 503 Service Unavailable Date: Sat, 12 Jan 2019 16:38:37 GMT Content-Length: 0 For this microservice just need to check the connection with redis because this service is very simple but depends on your microservice, you need to applied different checks.\nGithub Repo\n","permalink":"https://www.kungfudev.com/posts/health-checks/","summary":"According to the azure documentation in this excellent article, they state that.\n\u0026ldquo;It\u0026rsquo;s a good practice, and often a business requirement, to monitor web applications and back-end services, to ensure they\u0026rsquo;re available and performing correctly. However, it\u0026rsquo;s more difficult to monitor services running in the cloud than it is to monitor on-premises services.\u0026rdquo;\n\u0026ldquo;There are many factors that affect cloud-hosted applications such as network latency, the performance, and availability of the underlying compute and storage systems and the network bandwidth between them.","title":"Let's talk about Health Checks"},{"content":"Do you remember my last article where I wrote a service to look for a driver like uber? If not, you can check here So now, we going to write the V2 of our service.\nThe current state of our service, when a user consumes the resource \u0026lsquo;search\u0026rsquo;, the user receives a response with the closer driver to him. But what would happen if there are no drivers close to the user? We don\u0026rsquo;t want the service client doing a big amount of requests to the same endpoint to look for a driver. What we want to do is to follow the pattern Uber uses and that is that our client makes only one request and this request raises a task that looks for a driver to us for X time and, later on, the user receives the result.\nFor doing this we are going to use some of the tools Go provides us: Goroutines, Channels and time.Ticker struct.\nNOTE: This will be a basic implementation.\nGoroutine A goroutine is a lightweight, costing little more than the allocation of stack space. And the stacks start small, so they are cheap, and grow by allocating (and freeing) heap storage as required.\nTo run a function as a goroutine, just simply put the keyword go before a func call. When the func is done, the goroutine exits, silently.\ngo list.Sort() // run list.Sort concurrently; don\u0026#39;t wait for it. NOTE: Goroutines run in the same address space, so access to shared memory must be synchronized. The sync package provides useful primitives, although you won\u0026rsquo;t need them much in Go as there are other primitives.\nEffective Go - Goroutines Channels Channels are a typed conduit through which you can send and receive values with the channel operator, \u0026lt;-.\nch \u0026lt;- v // Send v to channel ch. v := \u0026lt;-ch // Receive from ch, and // assign value to v. (The data flows in the direction of the arrow.)\nLike maps, channels are allocated with make, and the resulting value acts as a reference to an underlying data structure. If an optional integer parameter is provided, it sets the buffer size for the channel. The default is zero, for an unbuffered or synchronous channel.\nci := make(chan int) // unbuffered channel of integers cj := make(chan int, 0) // unbuffered channel of integers cs := make(chan *os.File, 100) // buffered channel of pointers to Files Unbuffered channels combine communication—the exchange of a value—with synchronization—guaranteeing that two calculations (goroutines) are in a known state.\nThere are lots of nice idioms using channels. Here\u0026rsquo;s one to get us started. In the previous section we launched a sort in the background. A channel can allow the launching goroutine to wait for the sort to complete.\nc := make(chan int) // Allocate a channel. // Start the sort in a goroutine; when it completes, signal on the channel. go func() { list.Sort() c \u0026lt;- 1 // Send a signal; value does not matter. }() doSomethingForAWhile() \u0026lt;-c // Wait for sort to finish; discard sent value. Receivers always block until there is data to receive. If the channel is unbuffered, the sender blocks until the receiver has received the value. If the channel has a buffer, the sender blocks only until the value has been copied to the buffer; if the buffer is full, this means waiting until some receiver has retrieved a value.\nEffective Go - Channels time.Ticker Timers are for when you want to do something once in the future - tickers are for when you want to do something repeatedly at regular intervals. Here’s an example of a ticker that ticks periodically until we stop it.\npackage main import \u0026#34;time\u0026#34; import \u0026#34;fmt\u0026#34; func main() { // Tickers use a similar mechanism to timers: a // channel that is sent values. Here we\u0026#39;ll use the // `range` builtin on the channel to iterate over // the values as they arrive every 500ms. ticker := time.NewTicker(500 * time.Millisecond) go func() { for t := range ticker.C { fmt.Println(\u0026#34;Tick at\u0026#34;, t) } }() // Tickers can be stopped like timers. Once a ticker // is stopped it won\u0026#39;t receive any more values on its // channel. We\u0026#39;ll stop ours after 1600ms. time.Sleep(1600 * time.Millisecond) ticker.Stop() fmt.Println(\u0026#34;Ticker stopped\u0026#34;) } Let\u0026rsquo;s start coding First, we will create a new folder called tasks, inside we create a \u0026lsquo;request.go\u0026rsquo; that contains the code to do the search.\n// FILE: tasks/search.go // These are the reasons which a request is invalid. var ( ErrExpired = errors.New(\u0026#34;request expired\u0026#34;) ErrCanceled = errors.New(\u0026#34;request canceled\u0026#34;) ) // RequestDriverTask is a simple struct that contains info about the user, request and driver, you can add more information if you want. type RequestDriverTask struct { ID string UserID string Lat, Lng float64 DriverID string } // NewRequestDriverTask create and return a pointer to RequestDriverTask func NewRequestDriverTask(id, userID string, lat, lng float64) *RequestDriverTask { return \u0026amp;RequestDriverTask{ ID: id, UserID: userID, Lat:lat, Lng:lng, } } We implement the Run method, this func will be launched from the handler.\n// FILE: tasks/search.go // Run is the function for executing the task, this task validating the request and launches another goroutine called \u0026#39;doSearch\u0026#39; which does the search. func (r *RequestDriverTask) Run() { // We create a new ticker with 30s time duration, this it means that each 30s the task executes the search for a driver. ticker := time.NewTicker(time.Second * 30) // With the done channel, we receive if the driver was found done := make(chan struct{}) for { // The select statement lets a goroutine wait on multiple communication operations. select { case \u0026lt;-ticker.C: err := r.validateRequest() switch err { case nil: log.Println(fmt.Sprintf(\u0026#34;Search Driver - Request %s for Lat: %f and Lng: %f\u0026#34;, r.ID, r.Lat, r.Lng)) go r.doSearch(done) case ErrExpired: // Notify to user that the request expired. sendInfo(r, \u0026#34;Sorry, we did not find any driver.\u0026#34;) return case ErrCanceled: log.Printf(\u0026#34;Request %s has been canceled. \u0026#34;, r.ID) return default: // defensive programming: expected the unexpected log.Printf(\u0026#34;unexpected error: %v\u0026#34;, err) return } case _, ok := \u0026lt;-done: if !ok { sendInfo(r, fmt.Sprintf(\u0026#34;Driver %s found\u0026#34;, r.DriverID)) ticker.Stop() return } } } } Ok, now we going to create two methods for RequestDriverTask.\nThe first method is validateRequest, this function validates the key, if the key is active or if the key expired and will return error like a reason if the request is not valid.\nThe second method is doSearch, this function uses our RedisClient and its function SearchDrivers for doing search.\n// FILE: tasks/search.go // validateRequest validates if the request is valid and return an error like a reason in case not. func (r *RequestDriverTask) validateRequest() error { rClient := storages.GetRedisClient() keyValue, err := rClient.Get(r.ID).Result() if err != nil { // Request has been expired. return ErrExpired } isActive, _ := strconv.ParseBool(keyValue) if !isActive { // Request has been canceled. return ErrCanceled } return nil } // doSearch do search of driver and close to the channel. func (r *RequestDriverTask) doSearch(done chan struct{}) { rClient := storages.GetRedisClient() drivers := rClient.SearchDrivers(1, r.Lat, r.Lng, 5) if len(drivers) == 1 { // Driver found // Remove driver location, we can send a message to the driver for that it does not send again its location to this service. rClient.RemoveDriverLocation(drivers[0].Name) r.DriverID = drivers[0].Name close(done) } return } the function sendInfo is just example, you can implement another service or push notification or WebSocket if you want, I want to write another article where I implement an example using FCM with a little library that I wrote go-fcm to notify the user.\n// sendInfo this func is only example, you can use another services, websocket or push notification for send data to user. func sendInfo(r *RequestDriverTask, message string) { log.Println(\u0026#34;Message to user:\u0026#34;, r.UserID) log.Println(message) } Ok, we already have the functions for the search task, now we need to create the new endpoints for our service, we going to create a new folder into handler called \u0026lsquo;v2\u0026rsquo; and inside we create \u0026lsquo;search.go\u0026rsquo;\n// FILE: handler/v2/search.go func SearchV2(w http.ResponseWriter, r *http.Request) { rClient := storages.GetRedisClient() // We use Redis to keep a key unique for each request. // With this key also we will know if the request is active or if the user canceled the request. requestID, err := rClient.Incr(\u0026#34;request_id\u0026#34;).Result() if err != nil { return } key := strconv.Itoa(int(requestID)) // Set true value for the key and also the expiration time, this expiration time is the duration that has the request to find a driver. rClient.Set(key, true, time.Minute*4) body := struct { Lat, Lng float64 }{} if err := json.NewDecoder(r.Body).Decode(\u0026amp;body); err != nil { log.Printf(\u0026#34;could not decode request: %v\u0026#34;, err) http.Error(w, \u0026#34;could not decode request\u0026#34;, http.StatusInternalServerError) return } // We create a new task and launch with a goroutine. rTask := tasks.NewRequestDriverTask(key, fmt.Sprintf(\u0026#34;requestor_%s\u0026#34;, key), body.Lat, body.Lng) go rTask.Run() // Return 200 and request_id w.WriteHeader(http.StatusOK) w.Write([]byte(fmt.Sprintf(`{\u0026#34;request_id\u0026#34;: %s}`, key))) } Next, we create the handler for endpoint \u0026lsquo;v2/cancel\u0026rsquo; to cancel the request, because if the user doesn\u0026rsquo;t want to wait for the search it can cancel the request.\n// FILE: handler/v2/search.go func CancelRequest(w http.ResponseWriter, r *http.Request) { rClient := storages.GetRedisClient() body := struct { RequestID string `json:\u0026#34;request_id\u0026#34;` }{} if err := json.NewDecoder(r.Body).Decode(\u0026amp;body); err != nil { log.Printf(\u0026#34;could not decode request: %v\u0026#34;, err) http.Error(w, \u0026#34;could not decode request\u0026#34;, http.StatusInternalServerError) return } rClient.Set(body.RequestID, false, time.Minute*1) w.WriteHeader(http.StatusOK) return } Finally, we need to add new endpoints to our routes.\n// FILE: handler/base.go import ( \u0026#34;net/http\u0026#34; \u0026#34;github.com/douglasmakey/tracking/handler/v2\u0026#34; ) func NewHandler() *http.ServeMux { mux := http.NewServeMux() mux.HandleFunc(\u0026#34;/tracking\u0026#34;, tracking) mux.HandleFunc(\u0026#34;/search\u0026#34;, search) //V2 mux.HandleFunc(\u0026#34;/v2/search\u0026#34;, v2.SearchV2) mux.HandleFunc(\u0026#34;/v2/cancel\u0026#34;, v2.CancelRequest) return mux } Example Look up for the nearest driver\ncurl -i --header \u0026#34;Content-Type: application/json\u0026#34; --data \u0026#39;{\u0026#34;lat\u0026#34;: -33.44262, \u0026#34;lng\u0026#34;: -70.63054}\u0026#39; http://localhost:8000/v2/search HTTP/1.1 200 OK Date: Sat, 29 Sep 2018 15:33:48 GMT Content-Length: 17 Content-Type: application/json {\u0026#34;request_id\u0026#34;: 1} But if drivers are not close client location and pass 4 minutes or the time duration that we set, the request will be expired without finding any drivers.\nServer Log\n2018/09/30 01:57:53 Starting HTTP Server. Listening at \u0026#34;:8000\u0026#34; Search Driver - Request 1 for Lat: -33.442620 and Lng: -70.630540 Search Driver - Request 1 for Lat: -33.442620 and Lng: -70.630540 Search Driver - Request 1 for Lat: -33.442620 and Lng: -70.630540 Search Driver - Request 1 for Lat: -33.442620 and Lng: -70.630540 Search Driver - Request 1 for Lat: -33.442620 and Lng: -70.630540 Search Driver - Request 1 for Lat: -33.442620 and Lng: -70.630540 Search Driver - Request 1 for Lat: -33.442620 and Lng: -70.630540 Search Driver - Request 1 for Lat: -33.442620 and Lng: -70.630540 Message to user: requestor_1 Sorry, we did not find any driver. We going to do another request to \u0026lsquo;v2/search\u0026rsquo;, but after 1 minute in another terminal, we send driver location to service.\n// Another Terminal curl -i --header \u0026#34;Content-Type: application/json\u0026#34; --data \u0026#39;{\u0026#34;id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;lat\u0026#34;: -33.44091, \u0026#34;lng\u0026#34;: -70.6301}\u0026#39; http://localhost:8000/tracking // Terminal with main 2018/09/30 02:12:03 Starting HTTP Server. Listening at \u0026#34;:8000\u0026#34; 2018/09/30 02:12:38 Search Driver - Request 2 for Lat: -33.442620 and Lng: -70.630540 2018/09/30 02:13:08 Search Driver - Request 2 for Lat: -33.442620 and Lng: -70.630540 2018/09/30 02:13:38 Search Driver - Request 2 for Lat: -33.442620 and Lng: -70.630540 2018/09/30 02:13:38 Message to user: requestor_2 2018/09/30 02:13:38 Driver 1 found Ok, now we going to do another request to \u0026lsquo;v2/search\u0026rsquo;, but this time we going to do a request to \u0026lsquo;v2/cancel\u0026rsquo; to cancel the order because we can not wait.\n// Another Terminal curl -i --header \u0026#34;Content-Type: application/json\u0026#34; --data \u0026#39;{\u0026#34;request_id\u0026#34;: \u0026#34;3\u0026#34;}\u0026#39; http://localhost:8000/v2/cancel // Terminal with main 2018/09/30 02:19:24 Starting HTTP Server. Listening at \u0026#34;:8000\u0026#34; 2018/09/30 02:19:56 Search Driver - Request 3 for Lat: -33.442620 and Lng: -70.630540 2018/09/30 02:19:56 Request 3 has been canceled. ","permalink":"https://www.kungfudev.com/posts/tracking-service-v2/","summary":"Do you remember my last article where I wrote a service to look for a driver like uber? If not, you can check here So now, we going to write the V2 of our service.\nThe current state of our service, when a user consumes the resource \u0026lsquo;search\u0026rsquo;, the user receives a response with the closer driver to him. But what would happen if there are no drivers close to the user?","title":"Tracking Service with Go and Redis V2"},{"content":"Part 2: Tracking Service with Go and Redis V2\nImagine that we work at a startup like Uber and we need to create a new service that saves drivers locations every given time and processes it. This way, when someone requests a driver we can find out which drivers are closer to our picking point.\nThis is the core of our service. Save the locations and search nearby drivers. For this service we are using Go and Redis.\nRedis Redis is an open source (BSD licensed), in-memory data structure store, used as a database, cache and message broker. It supports data structures such as strings, hashes, lists, sets, sorted sets with range queries, bitmaps, hyperloglogs and geospatial indexes with radius queries. Redis\nRedis has multiple functions but for the purpose of this service we are going to focus on its geospatial functions.\nFirst we need to install Redis, I recommend using Docker running a container with Redis. By simply following this command, we will have a container running Redis in our machine.\ndocker run -d -p 6379:6379 redis Let\u0026rsquo;s start coding We are going to write a basic implementation for this service since I want to write other articles on how to improve this service. I will use this code as a base on my next articles.\nFor this service we need to use the package \u0026ldquo;github.com/go-redis/redis\u0026rdquo; that provides a Redis client for Golang.\nCreate a new project(folder) in your workdir. In my case I will call it \u0026rsquo;tracking\u0026rsquo;. First we need to install the package.\ngo get -u github.com/go-redis/redis Then we create the file \u0026lsquo;storages/redis.go\u0026rsquo; that contains the implementation that will help us getting a Redis client and some functions to work with geospatial.\nWe now create a struct that contains a pointer to the redis client. This pointer will have the functions that help us with this service, we also create a constant with the key name for our set in redis.\ntype RedisClient struct { *redis.Client } const key = \u0026#34;drivers\u0026#34; For the function to get the Redis client, we are going to use the singleton pattern with the help of the sync package and its Once.Do functionality.\nIn software engineering, the singleton pattern is a software design pattern that restricts the instantiation of a class to one object. This is useful when exactly one object is needed to coordinate actions across the system. If you want to read more about Singleton Pattern.\nBut how works Once.Do, the struct sync.Once has an atomic counter and it uses atomic.StoreUint32 to set a value to 1, when the function has been called, and then atomic.LoadUint32 to see if it needs to be called again. For this basic implementation GetRedisClient will be called from two endpoints but we only want to get one instance.\nvar once sync.Once var redisClient *RedisClient func GetRedisClient() *RedisClient { once.Do(func() { client := redis.NewClient(\u0026amp;redis.Options{ Addr: \u0026#34;localhost:6379\u0026#34;, Password: \u0026#34;\u0026#34;, // no password set DB: 0, // use default DB }) redisClient = \u0026amp;RedisClient{client} }) _, err := redisClient.Ping().Result() if err != nil { log.Fatalf(\u0026#34;Could not connect to redis %v\u0026#34;, err) } return redisClient } Then we create three functions for the RedisClient.\nAddDriverLocation: Add the specified geospatial item (latitude, longitude, name \u0026ldquo;in this case name is the driver id\u0026rdquo;) to the specified key, do you remember the key that we defined at the beginning for our Set in Redis ? This is it.\nfunc (c *RedisClient) AddDriverLocation(lng, lat float64, id string) { c.GeoAdd( key, \u0026amp;redis.GeoLocation{Longitude: lng, Latitude: lat, Name: id}, ) } RemoveDriverLocation: The client redis does not have the function GeoDel because GEODEL command does not exist, so we can use ZREM in order to remove elements. The Geo index structure is just a sorted set.\nfunc (c *RedisClient) RemoveDriverLocation(id string) { c.ZRem(key, id) } SearchDrivers: the function GeoRadius implements the command GEORADIUS that returns the members of a sorted set populated with geospatial information using GEOADD, which are within the borders of the area specified with the center location and the maximum distance from the center (the radius). If you want to learn more about this go GEORADIUS\nfunc (c *RedisClient) SearchDrivers(limit int, lat, lng, r float64) []redis.GeoLocation { /* WITHDIST: Also return the distance of the returned items from the specified center. The distance is returned in the same unit as the unit specified as the radius argument of the command. WITHCOORD: Also return the longitude,latitude coordinates of the matching items. WITHHASH: Also return the raw geohash-encoded sorted set score of the item, in the form of a 52 bit unsigned integer. This is only useful for low level hacks or debugging and is otherwise of little interest for the general user. */ res, _ := c.GeoRadius(key, lng, lat, \u0026amp;redis.GeoRadiusQuery{ Radius: r, Unit: \u0026#34;km\u0026#34;, WithGeoHash: true, WithCoord: true, WithDist: true, Count: limit, Sort: \u0026#34;ASC\u0026#34;, }).Result() return res } Next, create a main.go\npackage main import ( \u0026#34;net/http\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; ) func main() { // We create a simple httpserver server := http.Server{ Addr: fmt.Sprint(\u0026#34;:8000\u0026#34;), Handler: NewHandler(), } // Run server log.Printf(\u0026#34;Starting HTTP Server. Listening at %q\u0026#34;, server.Addr) if err := server.ListenAndServe(); err != nil { log.Printf(\u0026#34;%v\u0026#34;, err) } else { log.Println(\u0026#34;Server closed ! \u0026#34;) } } We create a simple server using http.Server.\nThen we create file \u0026lsquo;handler/handler.go\u0026rsquo; that contains the endpoints for our application.\nfunc NewHandler() *http.ServeMux { mux := http.NewServeMux() mux.HandleFunc(\u0026#34;tracking\u0026#34;, tracking) mux.HandleFunc(\u0026#34;search\u0026#34;, search) return mux } We use http.ServeMux to handle our endpoints, we create two endpoints for our service.\nThe first endpoint \u0026rsquo;tracking\u0026rsquo; let\u0026rsquo;s us save the last location sent from a driver, in this case we only want to save the last location. We could modify this endpoint so that previous locations are saved in another database\nfunc tracking(w http.ResponseWriter, r *http.Request) { // crate an anonymous struct for driver data. var driver = struct { ID string `json:\u0026#34;id\u0026#34;` Lat float64 `json:\u0026#34;lat\u0026#34;` Lng float64 `json:\u0026#34;lng\u0026#34;` }{} rClient := storages.GetRedisClient() if err := json.NewDecoder(r.Body).Decode(\u0026amp;driver); err != nil { log.Printf(\u0026#34;could not decode request: %v\u0026#34;, err) http.Error(w, \u0026#34;could not decode request\u0026#34;, http.StatusInternalServerError) return } // Add new location // You can save locations in another db rClient.AddDriverLocation(driver.Lng, driver.Lat, driver.ID) w.WriteHeader(http.StatusOK) return } The second endpoint is \u0026lsquo;search\u0026rsquo; with this endpoint we can find all drivers near a given point,\n// search receives lat and lng of the picking point and searches drivers about this point. func search(w http.ResponseWriter, r *http.Request) { rClient := storages.GetRedisClient() body := struct { Lat float64 `json:\u0026#34;lat\u0026#34;` Lng float64 `json:\u0026#34;lng\u0026#34;` Limit int `json:\u0026#34;limit\u0026#34;` }{} if err := json.NewDecoder(r.Body).Decode(\u0026amp;body); err != nil { log.Printf(\u0026#34;could not decode request: %v\u0026#34;, err) http.Error(w, \u0026#34;could not decode request\u0026#34;, http.StatusInternalServerError) return } drivers := rClient.SearchDrivers(body.Limit, body.Lat, body.Lng, 15) data, err := json.Marshal(drivers) if err != nil { http.Error(w, err.Error(), http.StatusInternalServerError) return } w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) w.WriteHeader(http.StatusOK) w.Write(data) return } Let\u0026rsquo;s test the service First, run the server.\ngo run main.go Next, we need to add four drivers locations.\nWe add four drivers as above in the map, the lines green show distance between picking point and drivers.\ncurl -i --header \u0026#34;Content-Type: application/json\u0026#34; --data \u0026#39;{\u0026#34;id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;lat\u0026#34;: -33.44091, \u0026#34;lng\u0026#34;: -70.6301}\u0026#39; http://localhost:8000/tracking curl -i --header \u0026#34;Content-Type: application/json\u0026#34; --data \u0026#39;{\u0026#34;id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;lat\u0026#34;: -33.44005, \u0026#34;lng\u0026#34;: -70.63279}\u0026#39; http://localhost:8000/tracking curl -i --header \u0026#34;Content-Type: application/json\u0026#34; --data \u0026#39;{\u0026#34;id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;lat\u0026#34;: -33.44338, \u0026#34;lng\u0026#34;: -70.63335}\u0026#39; http://localhost:8000/tracking curl -i --header \u0026#34;Content-Type: application/json\u0026#34; --data \u0026#39;{\u0026#34;id\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;lat\u0026#34;: -33.44186, \u0026#34;lng\u0026#34;: -70.62653}\u0026#39; http://localhost:8000/tracking Since we now have the locations of the drivers, we can do a spacial search.\nwe will look for 4 nearby drivers\ncurl -i --header \u0026#34;Content-Type: application/json\u0026#34; --data \u0026#39;{\u0026#34;lat\u0026#34;: -33.44262, \u0026#34;lng\u0026#34;: -70.63054, \u0026#34;limit\u0026#34;: 5}\u0026#39; http://localhost:8000/search As you will see the result matches with the map, see the lines greens in the map.\nHTTP/1.1 200 OK Content-Type: application/json Date: Wed, 08 Aug 2018 05:07:57 GMT Content-Length: 456 [ { \u0026#34;Name\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;Longitude\u0026#34;: -70.63009768724442, \u0026#34;Latitude\u0026#34;: -33.44090957099124, \u0026#34;Dist\u0026#34;: 0.1946, \u0026#34;GeoHash\u0026#34;: 861185092131738 }, { \u0026#34;Name\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;Longitude\u0026#34;: -70.63334852457047, \u0026#34;Latitude\u0026#34;: -33.44338092412159, \u0026#34;Dist\u0026#34;: 0.2741, \u0026#34;GeoHash\u0026#34;: 861185074815667 }, { \u0026#34;Name\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;Longitude\u0026#34;: -70.63279062509537, \u0026#34;Latitude\u0026#34;: -33.44005030051822, \u0026#34;Dist\u0026#34;: 0.354, \u0026#34;GeoHash\u0026#34;: 861185086448695 }, { \u0026#34;Name\u0026#34;: \u0026#34;4\u0026#34;, \u0026#34;Longitude\u0026#34;: -70.62653034925461, \u0026#34;Latitude\u0026#34;: -33.44186009142599, \u0026#34;Dist\u0026#34;: 0.3816, \u0026#34;GeoHash\u0026#34;: 861185081504625 } ] Look up for the nearest driver\ncurl -i --header \u0026#34;Content-Type: application/json\u0026#34; --data \u0026#39;{\u0026#34;lat\u0026#34;: -33.44262, \u0026#34;lng\u0026#34;: -70.63054, \u0026#34;limit\u0026#34;: 1}\u0026#39; http://localhost:8000/search Result\nHTTP/1.1 200 OK Content-Type: application/json Date: Wed, 08 Aug 2018 05:12:24 GMT Content-Length: 115 [{\u0026#34;Name\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;Longitude\u0026#34;:-70.63009768724442,\u0026#34;Latitude\u0026#34;:-33.44090957099124,\u0026#34;Dist\u0026#34;:0.1946,\u0026#34;GeoHash\u0026#34;:861185092131738}] Github Repo\n","permalink":"https://www.kungfudev.com/posts/tracking-service-with-go-and-redis/","summary":"Part 2: Tracking Service with Go and Redis V2\nImagine that we work at a startup like Uber and we need to create a new service that saves drivers locations every given time and processes it. This way, when someone requests a driver we can find out which drivers are closer to our picking point.\nThis is the core of our service. Save the locations and search nearby drivers. For this service we are using Go and Redis.","title":"Tracking Service with Go and Redis"},{"content":"Authentication is the most common part in any application. You can implement your own authentication system or use one of the many alternatives that exist, but in this case we are going to use OAuth2.\nOAuth is a specification that allows users to delegate access to their data without sharing their username and password with that service, if you want to read more about Oauth2 go here.\nConfig Google Project First things first, we need to create our Google Project and create OAuth2 credentials.\nGo to Google Cloud Platform Create a new project or select one if you already have it. Go to Credentials and then create a new one choosing “OAuth client ID” Add \u0026ldquo;authorized redirect URL\u0026rdquo;, for this example localhost:8000/auth/google/callback Copy the client_id and client secret How OAuth2 works with Google The authorization sequence begins when your application redirects the browser to a Google URL; the URL includes query parameters that indicate the type of access being requested. Google handles the user authentication, session selection, and user consent. The result is an authorization code, which the application can exchange for an access token and a refresh token.\nThe application should store the refresh token for future use and use the access token to access a Google API. Once the access token expires, the application uses the refresh token to obtain a new one.\nLet\u0026rsquo;s go to the code We will use the package \u0026ldquo;golang.org/x/oauth2\u0026rdquo; that provides support for making OAuth2 authorized and authenticated HTTP requests.\nCreate a new project(folder) in your workdir in my case I will call it \u0026lsquo;oauth2-example\u0026rsquo;, and we need to include the package of oauth2.\ngo get golang.org/x/oauth2\nSo into the project we create a main.go.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/douglasmakey/oauth2-example/handlers\u0026#34; ) func main() { server := \u0026amp;http.Server{ Addr: fmt.Sprintf(\u0026#34;:8000\u0026#34;), Handler: handlers.New(), } log.Printf(\u0026#34;Starting HTTP Server. Listening at %q\u0026#34;, server.Addr) if err := server.ListenAndServe(); err != http.ErrServerClosed { log.Printf(\u0026#34;%v\u0026#34;, err) } else { log.Println(\u0026#34;Server closed!\u0026#34;) } } We create a simple server using http.Server and run.\nNext, we create folder \u0026lsquo;handlers\u0026rsquo; that contains handler of our application, in this folder create \u0026lsquo;base.go\u0026rsquo;.\npackage handlers import ( \u0026#34;net/http\u0026#34; ) func New() http.Handler { mux := http.NewServeMux() // Root mux.Handle(\u0026#34;/\u0026#34;, http.FileServer(http.Dir(\u0026#34;templates/\u0026#34;))) // OauthGoogle mux.HandleFunc(\u0026#34;/auth/google/login\u0026#34;, oauthGoogleLogin) mux.HandleFunc(\u0026#34;/auth/google/callback\u0026#34;, oauthGoogleCallback) return mux } We use http.ServeMux to handle our endpoints, next we create the Root endpoint \u0026ldquo;/\u0026rdquo; for serving a simple template with a minimmum HTML\u0026amp;CSS in this example we use \u0026lsquo;http. http.FileServer\u0026rsquo;, that template is \u0026lsquo;index.html\u0026rsquo; and is in the folder \u0026rsquo;templates\u0026rsquo;.\nAlso we create two endpoints for Oauth with Google \u0026ldquo;/auth/google/login\u0026rdquo; and \u0026ldquo;/auth/google/callback\u0026rdquo;. Remember when we configured our application in the Google console? The callback url must be the same.\nNext, we create another file into handlers, we\u0026rsquo;ll call it \u0026lsquo;oauth_google.go\u0026rsquo;, this file contains all logic to handle OAuth with Google in our application.\nWe Declare the var googleOauthConfig with auth.Config to communicate with Google. Scopes: OAuth 2.0 scopes provide a way to limit the amount of access that is granted to an access token.\nvar googleOauthConfig = \u0026amp;oauth2.Config{ RedirectURL: \u0026#34;http://localhost:8000/auth/google/callback\u0026#34;, ClientID: os.Getenv(\u0026#34;GOOGLE_OAUTH_CLIENT_ID\u0026#34;), ClientSecret: os.Getenv(\u0026#34;GOOGLE_OAUTH_CLIENT_SECRET\u0026#34;), Scopes: []string{\u0026#34;https://www.googleapis.com/auth/userinfo.email\u0026#34;}, Endpoint: google.Endpoint, } Handler oauthGoogleLogin This handler creates a login link and redirects the user to it:\nAuthCodeURL receive state that is a token to protect the user from CSRF attacks. You must always provide a non-empty string and validate that it matches with the state query parameter on your redirect callback, It\u0026rsquo;s advisable that this is randomly generated for each request, that\u0026rsquo;s why we use a simple cookie.\nfunc oauthGoogleLogin(w http.ResponseWriter, r *http.Request) { // Create oauthState cookie oauthState := generateStateOauthCookie(w) u := googleOauthConfig.AuthCodeURL(oauthState) http.Redirect(w, r, u, http.StatusTemporaryRedirect) } func generateStateOauthCookie(w http.ResponseWriter) string { var expiration = time.Now().Add(365 * 24 * time.Hour) b := make([]byte, 16) rand.Read(b) state := base64.URLEncoding.EncodeToString(b) cookie := http.Cookie{Name: \u0026#34;oauthstate\u0026#34;, Value: state, Expires: expiration} http.SetCookie(w, \u0026amp;cookie) return state } Handler oauthGoogleCallback This handler check if the state is equals to oauthStateCookie, and pass the code to the function getUserDataFromGoogle.\nfunc oauthGoogleCallback(w http.ResponseWriter, r *http.Request) { // Read oauthState from Cookie oauthState, _ := r.Cookie(\u0026#34;oauthstate\u0026#34;) if r.FormValue(\u0026#34;state\u0026#34;) != oauthState.Value { log.Println(\u0026#34;invalid oauth google state\u0026#34;) http.Redirect(w, r, \u0026#34;/\u0026#34;, http.StatusTemporaryRedirect) return } data, err := getUserDataFromGoogle(r.FormValue(\u0026#34;code\u0026#34;)) if err != nil { log.Println(err.Error()) http.Redirect(w, r, \u0026#34;/\u0026#34;, http.StatusTemporaryRedirect) return } // GetOrCreate User in your db. // Redirect or response with a token. // More code ..... fmt.Fprintf(w, \u0026#34;UserInfo: %s\\n\u0026#34;, data) } func getUserDataFromGoogle(code string) ([]byte, error) { // Use code to get token and get user info from Google. token, err := googleOauthConfig.Exchange(context.Background(), code) if err != nil { return nil, fmt.Errorf(\u0026#34;code exchange wrong: %s\u0026#34;, err.Error()) } response, err := http.Get(oauthGoogleUrlAPI + token.AccessToken) if err != nil { return nil, fmt.Errorf(\u0026#34;failed getting user info: %s\u0026#34;, err.Error()) } defer response.Body.Close() contents, err := ioutil.ReadAll(response.Body) if err != nil { return nil, fmt.Errorf(\u0026#34;failed read response: %s\u0026#34;, err.Error()) } return contents, nil } Full code oauth_google.go package handlers import ( \u0026#34;golang.org/x/oauth2\u0026#34; \u0026#34;golang.org/x/oauth2/google\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;context\u0026#34; \u0026#34;log\u0026#34; \u0026#34;encoding/base64\u0026#34; \u0026#34;crypto/rand\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; ) // Scopes: OAuth 2.0 scopes provide a way to limit the amount of access that is granted to an access token. var googleOauthConfig = \u0026amp;oauth2.Config{ RedirectURL: \u0026#34;http://localhost:8000/auth/google/callback\u0026#34;, ClientID: os.Getenv(\u0026#34;GOOGLE_OAUTH_CLIENT_ID\u0026#34;), ClientSecret: os.Getenv(\u0026#34;GOOGLE_OAUTH_CLIENT_SECRET\u0026#34;), Scopes: []string{\u0026#34;https://www.googleapis.com/auth/userinfo.email\u0026#34;}, Endpoint: google.Endpoint, } const oauthGoogleUrlAPI = \u0026#34;https://www.googleapis.com/oauth2/v2/userinfo?access_token=\u0026#34; func oauthGoogleLogin(w http.ResponseWriter, r *http.Request) { // Create oauthState cookie oauthState := generateStateOauthCookie(w) /* AuthCodeURL receive state that is a token to protect the user from CSRF attacks. You must always provide a non-empty string and validate that it matches the the state query parameter on your redirect callback. */ u := googleOauthConfig.AuthCodeURL(oauthState) http.Redirect(w, r, u, http.StatusTemporaryRedirect) } func oauthGoogleCallback(w http.ResponseWriter, r *http.Request) { // Read oauthState from Cookie oauthState, _ := r.Cookie(\u0026#34;oauthstate\u0026#34;) if r.FormValue(\u0026#34;state\u0026#34;) != oauthState.Value { log.Println(\u0026#34;invalid oauth google state\u0026#34;) http.Redirect(w, r, \u0026#34;/\u0026#34;, http.StatusTemporaryRedirect) return } data, err := getUserDataFromGoogle(r.FormValue(\u0026#34;code\u0026#34;)) if err != nil { log.Println(err.Error()) http.Redirect(w, r, \u0026#34;/\u0026#34;, http.StatusTemporaryRedirect) return } // GetOrCreate User in your db. // Redirect or response with a token. // More code ..... fmt.Fprintf(w, \u0026#34;UserInfo: %s\\n\u0026#34;, data) } func generateStateOauthCookie(w http.ResponseWriter) string { var expiration = time.Now().Add(365 * 24 * time.Hour) b := make([]byte, 16) rand.Read(b) state := base64.URLEncoding.EncodeToString(b) cookie := http.Cookie{Name: \u0026#34;oauthstate\u0026#34;, Value: state, Expires: expiration} http.SetCookie(w, \u0026amp;cookie) return state } func getUserDataFromGoogle(code string) ([]byte, error) { // Use code to get token and get user info from Google. token, err := googleOauthConfig.Exchange(context.Background(), code) if err != nil { return nil, fmt.Errorf(\u0026#34;code exchange wrong: %s\u0026#34;, err.Error()) } response, err := http.Get(oauthGoogleUrlAPI + token.AccessToken) if err != nil { return nil, fmt.Errorf(\u0026#34;failed getting user info: %s\u0026#34;, err.Error()) } defer response.Body.Close() contents, err := ioutil.ReadAll(response.Body) if err != nil { return nil, fmt.Errorf(\u0026#34;failed read response: %s\u0026#34;, err.Error()) } return contents, nil } let\u0026rsquo;s run and test go run main.go Repository with code Repo\n","permalink":"https://www.kungfudev.com/posts/oauth2-example-with-go/","summary":"Authentication is the most common part in any application. You can implement your own authentication system or use one of the many alternatives that exist, but in this case we are going to use OAuth2.\nOAuth is a specification that allows users to delegate access to their data without sharing their username and password with that service, if you want to read more about Oauth2 go here.\nConfig Google Project First things first, we need to create our Google Project and create OAuth2 credentials.","title":"Oauth2 with Google in Go"},{"content":"I’m writing this article because I recently did this exercise in HackerRank:\nI don’t have a degree in computer science or similar (but i’m working as a software engineer the last 3 years) so I really don’t have a solid math knowledge and at first sight this exercise seems easy, right?\nMost of you will think that it is, but to me it wasn’t. My first approach was to think in a logical solution (at least I’d like to think that it was), then I thinked that if i wanted to know if the kangaroos will ever land on the same location at the same time I would have to move the kangaroos until they were both on the same location\nI designed a loop that would move the kangaroos and check if they are on the same position.\nForgetting other validations, I only wrote the necessary code for the case:\nfor i:=0 ; i\u0026lt;10000; i++ { kOnePosition = moveKangoro(x1, v1) kTwoPosition = moveKangoro(x2, v2) if kOnePosition == kTwoPosition { fmt.Println(“YES”) break; } } I wrote variants of this code and other ones, but neither of them passed the tests. After 10 or 20 minutes I thought in a math solution.\nThe equation to verify if in n jumps the kangaroos are going to be on the same location is this:\nx1 + (v1 * n) = x2 + (v2 * n) -\u0026gt; where n is of num of jumps.\nSo, we are going to resolve this equation:\n(v1 * n) — (v2 * n) = x2 — x1\nn(v1 — v2) = x2 — x1\nn = (x2 — x1) / (v1 — v2)\nWe know that the value of n needs to be an integer, so we’re going to replace the division by the modular division and check if the operation leaves a remainder of 0\n(x2 — x1) % (v1 — v2) == 0\nThis means that for a set of initial positions and meters per jump, we can know that the kangaroos are going to be on the same position at the same time only if the remainder value is 0.\nAgain, I’m forgetting other validations, I only wrote the necessary code for the case:\nif x1 == x2 \u0026amp;\u0026amp; v1 == v2 { fmt.Println(\u0026#34;YES\u0026#34;) } else if x1 == x2 \u0026amp;\u0026amp; v1 != v2{ fmt.Println(\u0026#34;NO\u0026#34;) } else { if v1 \u0026gt; v2 \u0026amp;\u0026amp; ((x2-x1)%(v1-v2)) == 0 { fmt.Println(\u0026#34;YES\u0026#34;) } else { fmt.Println(\u0026#34;NO\u0026#34;) } } This algorithm passed all the tests and we could only do this efficiently with maths.\nSometimes I hear developers say “Math is not very important” or phrases like “A good coder is bad at math” and I think that they are wrong because as we can saw in this case, maths helps us to solve a problem in an efficient way.\nI’ve a lot to learn and It’s exciting to find these problems to improve every day.\nCode in Github\n","permalink":"https://www.kungfudev.com/posts/how-important-is-math-in-computer-programming/","summary":"I’m writing this article because I recently did this exercise in HackerRank:\nI don’t have a degree in computer science or similar (but i’m working as a software engineer the last 3 years) so I really don’t have a solid math knowledge and at first sight this exercise seems easy, right?\nMost of you will think that it is, but to me it wasn’t. My first approach was to think in a logical solution (at least I’d like to think that it was), then I thinked that if i wanted to know if the kangaroos will ever land on the same location at the same time I would have to move the kangaroos until they were both on the same location","title":"How important is math in computer programming?"}]